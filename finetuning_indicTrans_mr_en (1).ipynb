{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdmaTWLA_R4z"
      },
      "source": [
        "####Fine-tuning en-indic model of indicTrans:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W5-SBIxZepM",
        "outputId": "a3f3710a-b38d-4a3c-f47e-2f382af07862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu117\n",
            "Collecting torch==1.13.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (1801.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m960.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp310-cp310-linux_x86_64.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.13.1\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchaudio-0.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu117) (4.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (1.22.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2.0.12)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.1+cu118\n",
            "    Uninstalling torchvision-0.15.1+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.1+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.1+cu118\n",
            "    Uninstalling torchaudio-2.0.1+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1+cu117 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1+cu117 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.1+cu117 torchaudio-0.13.1+cu117 torchvision-0.14.1+cu117\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Exj0v-AzancI"
      },
      "outputs": [],
      "source": [
        "#Making a separate directory for keeping finetuning data and model\n",
        "\n",
        "!mkdir /content/finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6Yne1cdAMnS",
        "outputId": "6a103a41-ecef-4e28-c8c1-ae41765032c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetuning\n"
          ]
        }
      ],
      "source": [
        "%cd /content/finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YpzLNQEaNx_",
        "outputId": "5bc4e0bd-1c75-4692-8c03-08d1d7319571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indicTrans'...\n",
            "remote: Enumerating objects: 697, done.\u001b[K\n",
            "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
            "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
            "remote: Total 697 (delta 278), reused 344 (delta 240), pack-reused 297\u001b[K\n",
            "Receiving objects: 100% (697/697), 2.64 MiB | 10.89 MiB/s, done.\n",
            "Resolving deltas: 100% (405/405), done.\n",
            "/content/finetuning/indicTrans\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1362, done.\u001b[K\n",
            "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 1362 (delta 111), reused 98 (delta 93), pack-reused 1219\u001b[K\n",
            "Receiving objects: 100% (1362/1362), 9.56 MiB | 15.48 MiB/s, done.\n",
            "Resolving deltas: 100% (721/721), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 27.36 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Updating files: 100% (28/28), done.\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 597, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 597 (delta 8), reused 12 (delta 4), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (597/597), 252.23 KiB | 7.88 MiB/s, done.\n",
            "Resolving deltas: 100% (357/357), done.\n",
            "/content/finetuning\n"
          ]
        }
      ],
      "source": [
        "# clone the repo for running evaluation\n",
        "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
        "%cd indicTrans\n",
        "# clone requirements repositories\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zylyE9sUaTvT",
        "outputId": "fe066615-804f-4d88-b621-c71793fc822d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 43.0 kB of archives.\n",
            "After this operation, 115 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tree amd64 1.8.0-1 [43.0 kB]\n",
            "Fetched 43.0 kB in 1s (84.3 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.8.0-1_amd64.deb ...\n",
            "Unpacking tree (1.8.0-1) ...\n",
            "Setting up tree (1.8.0-1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting mock\n",
            "  Downloading mock-5.0.2-py3-none-any.whl (30 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (9.0.0)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.91-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.2)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Collecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Collecting sphinx-argparse\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme\n",
            "  Downloading sphinx_rtd_theme-1.2.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (3.5.4)\n",
            "Requirement already satisfied: docutils<0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.16)\n",
            "Collecting sphinxcontrib-jquery!=3.0.0,>=2.0.0\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.4)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.13)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (67.7.2)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.27.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.3)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.12.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.26.15)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=564fc3a8215f3eb73a2462f8feb6125b290f6ac85f4a4ab24cb6ae1740d4e932\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: morfessor, tensorboardX, sacremoses, portalocker, mock, colorama, sacrebleu, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed colorama-0.4.6 indic-nlp-library-0.91 mock-5.0.2 morfessor-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1 sacremoses-0.0.53 sphinx-argparse-0.4.0 sphinx-rtd-theme-1.2.0 sphinxcontrib-jquery-4.1 tensorboardX-2.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mosestokenizer\n",
            "  Downloading mosestokenizer-1.2.1.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openfile\n",
            "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
            "Collecting uctools\n",
            "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting toolwrapper\n",
            "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.10/dist-packages (from subword-nmt) (5.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from subword-nmt) (4.65.0)\n",
            "Building wheels for collected packages: mosestokenizer, docopt, toolwrapper, uctools\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mosestokenizer: filename=mosestokenizer-1.2.1-py3-none-any.whl size=49186 sha256=35cec9571ecb4f332fc4539c2942841388943f9bb1d85376214c1c4895d2c641\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d8/15/4c5ebbe883513f003cb055a0369c77c9df857023a706f39e70\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=c0ac56e35ce3a00f428fb2e798650a8677a6a9bcb64b1a0349166ffad606e44a\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3350 sha256=b11a1e5fb4539833456087e138fe0645f8d5d21a707dbe5d94c19586069f77cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/af/b1/99b57a06dda78fdcee86d2e22c64743f3b8df8f31cfc04baf7\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6160 sha256=832471f237d493743d3e40747b2c9c131de2f5391d50b7eb41e6e03e44c4e542\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/ee/10/33257b0801ac6a912c841939032c16da1eb3db377afe1443e5\n",
            "Successfully built mosestokenizer docopt toolwrapper uctools\n",
            "Installing collected packages: toolwrapper, openfile, docopt, uctools, subword-nmt, mosestokenizer\n",
            "Successfully installed docopt-0.6.2 mosestokenizer-1.2.1 openfile-0.0.7 subword-nmt-0.3.8 toolwrapper-2.1.0 uctools-1.3.0\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 34544, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 34544 (delta 0), reused 1 (delta 0), pack-reused 34543\u001b[K\n",
            "Receiving objects: 100% (34544/34544), 24.05 MiB | 17.54 MiB/s, done.\n",
            "Resolving deltas: 100% (25092/25092), done.\n",
            "/content/finetuning/fairseq\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/finetuning/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (0.29.34)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.22.4)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.15.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2022.10.31)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (0.13.1+cu117)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (23.1)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.13.1+cu117)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.7/272.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.2.2)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.7.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.8.10)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.1.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=21479692 sha256=6f30adadf3d6f77ade32db6a9c8a3bc0ea8bafbe14070f58b40f306a9089fce6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xqr9fbpd/wheels/31/09/62/01f91bc6958b01e6f9867ecf5e28929a376a12e25ce4a134fe\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=86d0202fa0bed93cecf590afeb604488994701080bd420b327b6c1e852671f55\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, omegaconf, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.7.3 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.19-cp310-cp310-manylinux2014_x86_64.whl (108.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.2/108.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==2.0.0\n",
            "  Using cached torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "Collecting pyre-extensions==0.0.29\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (4.5.0)\n",
            "Collecting typing-inspect\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
            "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.1)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58\n",
            "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91\n",
            "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
            "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.1.2)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
            "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3\n",
            "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (2.0.0)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91\n",
            "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->xformers) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->xformers) (0.40.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->xformers) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->xformers) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->xformers) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->xformers) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, mypy-extensions, typing-inspect, nvidia-cusolver-cu11, nvidia-cudnn-cu11, pyre-extensions, torch, xformers\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu117\n",
            "    Uninstalling torch-1.13.1+cu117:\n",
            "      Successfully uninstalled torch-1.13.1+cu117\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu117 requires torch==1.13.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchaudio 0.13.1+cu117 requires torch==1.13.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mypy-extensions-1.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pyre-extensions-0.0.29 torch-2.0.0 typing-inspect-0.8.0 xformers-0.0.19\n",
            "/content/finetuning\n"
          ]
        }
      ],
      "source": [
        "! sudo apt install tree\n",
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "! pip install mosestokenizer subword-nmt\n",
        "# Install fairseq from source\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "%cd fairseq\n",
        "# !git checkout da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d\n",
        "!pip install ./\n",
        "! pip install xformers\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XkB4yRHaWpe"
      },
      "outputs": [],
      "source": [
        "# add fairseq folder to python path\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O942YOwwa34T",
        "outputId": "b9b7cf1e-b950-4828-b5bb-d622a7a23a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-04 07:17:34--  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/indic2en.zip\n",
            "Resolving ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)... 101.53.136.19, 164.52.210.97, 101.53.152.30, ...\n",
            "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|101.53.136.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4759117228 (4.4G) [application/zip]\n",
            "Saving to: ‘indic2en.zip’\n",
            "\n",
            "indic2en.zip        100%[===================>]   4.43G  11.2MB/s    in 6m 48s  \n",
            "\n",
            "2023-05-04 07:24:24 (11.1 MB/s) - ‘indic2en.zip’ saved [4759117228/4759117228]\n",
            "\n",
            "Archive:  indic2en.zip\n",
            "   creating: indic-en/\n",
            "   creating: indic-en/vocab/\n",
            "  inflating: indic-en/vocab/bpe_codes.32k.SRC  \n",
            "  inflating: indic-en/vocab/vocab.SRC  \n",
            "  inflating: indic-en/vocab/vocab.TGT  \n",
            "  inflating: indic-en/vocab/bpe_codes.32k.TGT  \n",
            "   creating: indic-en/final_bin/\n",
            "  inflating: indic-en/final_bin/preprocess.log  \n",
            "  inflating: indic-en/final_bin/dict.TGT.txt  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/dict.SRC.txt  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.SRC.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.SRC.bin  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.SRC.bin  \n",
            "   creating: indic-en/model/\n",
            "  inflating: indic-en/model/checkpoint_best.pt  \n"
          ]
        }
      ],
      "source": [
        "# downloading the en-indic model\n",
        "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/indic2en.zip\n",
        "!unzip indic2en.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm-PEHOzaa6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf65019f-5c32-45fa-f5dd-a636169c53bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetuning\n",
            "--2023-05-04 07:25:39--  https://drive.google.com/uc?id=19rn5bsCi_VU8aR4Yl0FYu-Y3f06_L7tg&export=download\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.107.113, 142.250.107.138, 142.250.107.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.107.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sj8bskmsa9iq6ad72uls3pk48cln28kv/1683185100000/10066636681463159322/*/19rn5bsCi_VU8aR4Yl0FYu-Y3f06_L7tg?e=download&uuid=073b67ec-8aa1-4bd2-b9b8-2d313f0b7d00 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-05-04 07:25:41--  https://doc-10-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sj8bskmsa9iq6ad72uls3pk48cln28kv/1683185100000/10066636681463159322/*/19rn5bsCi_VU8aR4Yl0FYu-Y3f06_L7tg?e=download&uuid=073b67ec-8aa1-4bd2-b9b8-2d313f0b7d00\n",
            "Resolving doc-10-a0-docs.googleusercontent.com (doc-10-a0-docs.googleusercontent.com)... 173.194.203.132, 2607:f8b0:400e:c05::84\n",
            "Connecting to doc-10-a0-docs.googleusercontent.com (doc-10-a0-docs.googleusercontent.com)|173.194.203.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1123772 (1.1M) [application/x-zip-compressed]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>]   1.07M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-05-04 07:25:41 (96.7 MB/s) - ‘dataset.zip’ saved [1123772/1123772]\n",
            "\n",
            "Archive:  dataset.zip\n",
            "   creating: dataset_finetune/\n",
            "   creating: dataset_finetune/dev/\n",
            "  inflating: dataset_finetune/dev/dev.en  \n",
            "  inflating: dataset_finetune/dev/dev.mr  \n",
            "   creating: dataset_finetune/test/\n",
            "  inflating: dataset_finetune/test/test.en  \n",
            "  inflating: dataset_finetune/test/test.mr  \n",
            "   creating: dataset_finetune/train/\n",
            "   creating: dataset_finetune/train/en-mr/\n",
            "  inflating: dataset_finetune/train/en-mr/train.en  \n",
            "  inflating: dataset_finetune/train/en-mr/train.mr  \n",
            "   creating: dataset_finetune/train/mr-en/\n",
            "  inflating: dataset_finetune/train/mr-en/train.en  \n",
            "  inflating: dataset_finetune/train/mr-en/train.mr  \n",
            "/content/finetuning/indicTrans\n"
          ]
        }
      ],
      "source": [
        "%cd /content/finetuning\n",
        "!wget \"https://drive.google.com/uc?id=19rn5bsCi_VU8aR4Yl0FYu-Y3f06_L7tg&export=download\" -O dataset.zip\n",
        "!unzip dataset.zip\n",
        "%cd /content/finetuning/indicTrans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCNDq3szds5a",
        "outputId": "eb6b6add-e258-4b12-a496-0ac9d0a2bcdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment /content/finetuning/dataset_finetune on mr to en\n",
            "Applying normalization and script conversion for train \n",
            "100% 5168/5168 [00:00<00:00, 10012.02it/s]\n",
            "100% 5168/5168 [00:01<00:00, 2825.64it/s]\n",
            "Number of sentences in train : 5168\n",
            "Applying normalization and script conversion for dev \n",
            "100% 1000/1000 [00:00<00:00, 4951.35it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1371.81it/s]\n",
            "Number of sentences in dev : 1000\n",
            "Applying normalization and script conversion for test \n",
            "100% 1000/1000 [00:00<00:00, 3896.86it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1386.31it/s]\n",
            "Number of sentences in test : 1000\n",
            "\n",
            "/content/finetuning/dataset_finetune/data/train.SRC\n",
            "/content/finetuning/dataset_finetune/data/train.TGT\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "src: bn, tgt:en\n",
            "src: gu, tgt:en\n",
            "src: hi, tgt:en\n",
            "src: kn, tgt:en\n",
            "src: ml, tgt:en\n",
            "src: mr, tgt:en\n",
            "/content/finetuning/dataset_finetune/norm/mr-en/train.mr\n",
            "/content/finetuning/dataset_finetune/norm/mr-en/train.en\n",
            "src: or, tgt:en\n",
            "src: pa, tgt:en\n",
            "src: ta, tgt:en\n",
            "src: te, tgt:en\n",
            "100% 11/11 [00:00<00:00, 1184.62it/s]\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "src: bn, tgt:en\n",
            "src: gu, tgt:en\n",
            "src: hi, tgt:en\n",
            "src: kn, tgt:en\n",
            "src: ml, tgt:en\n",
            "src: mr, tgt:en\n",
            "/content/finetuning/dataset_finetune/norm/mr-en/train.mr\n",
            "src: or, tgt:en\n",
            "src: pa, tgt:en\n",
            "src: ta, tgt:en\n",
            "src: te, tgt:en\n",
            "100% 11/11 [00:00<00:00, 2909.59it/s]\n",
            "\n",
            "/content/finetuning/dataset_finetune/data/dev.SRC\n",
            "/content/finetuning/dataset_finetune/data/dev.TGT\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "src: bn, tgt:en\n",
            "src: gu, tgt:en\n",
            "src: hi, tgt:en\n",
            "src: kn, tgt:en\n",
            "src: ml, tgt:en\n",
            "src: mr, tgt:en\n",
            "/content/finetuning/dataset_finetune/norm/mr-en/dev.mr\n",
            "/content/finetuning/dataset_finetune/norm/mr-en/dev.en\n",
            "src: or, tgt:en\n",
            "src: pa, tgt:en\n",
            "src: ta, tgt:en\n",
            "src: te, tgt:en\n",
            "100% 11/11 [00:00<00:00, 2999.05it/s]\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "src: bn, tgt:en\n",
            "src: gu, tgt:en\n",
            "src: hi, tgt:en\n",
            "src: kn, tgt:en\n",
            "src: ml, tgt:en\n",
            "src: mr, tgt:en\n",
            "/content/finetuning/dataset_finetune/norm/mr-en/dev.mr\n",
            "src: or, tgt:en\n",
            "src: pa, tgt:en\n",
            "src: ta, tgt:en\n",
            "src: te, tgt:en\n",
            "100% 11/11 [00:00<00:00, 11491.24it/s]\n",
            "\n",
            "/content/finetuning/dataset_finetune/data/test.SRC\n",
            "/content/finetuning/dataset_finetune/data/test.TGT\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "src: bn, tgt:en\n",
            "src: gu, tgt:en\n",
            "src: hi, tgt:en\n",
            "src: kn, tgt:en\n",
            "src: ml, tgt:en\n",
            "src: mr, tgt:en\n",
            "/content/finetuning/dataset_finetune/norm/mr-en/test.mr\n",
            "/content/finetuning/dataset_finetune/norm/mr-en/test.en\n",
            "src: or, tgt:en\n",
            "src: pa, tgt:en\n",
            "src: ta, tgt:en\n",
            "src: te, tgt:en\n",
            "100% 11/11 [00:00<00:00, 3304.49it/s]\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "src: bn, tgt:en\n",
            "src: gu, tgt:en\n",
            "src: hi, tgt:en\n",
            "src: kn, tgt:en\n",
            "src: ml, tgt:en\n",
            "src: mr, tgt:en\n",
            "/content/finetuning/dataset_finetune/norm/mr-en/test.mr\n",
            "src: or, tgt:en\n",
            "src: pa, tgt:en\n",
            "src: ta, tgt:en\n",
            "src: te, tgt:en\n",
            "100% 11/11 [00:00<00:00, 9528.57it/s]\n",
            "Applying bpe to the new finetuning data\n",
            "train\n",
            "Apply to SRC corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "dev\n",
            "Apply to SRC corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "test\n",
            "Apply to SRC corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Adding language tags\n",
            "5168it [00:00, 222948.45it/s]\n",
            "1000it [00:00, 233692.00it/s]\n",
            "1000it [00:00, 152437.00it/s]\n",
            "Binarizing data\n",
            "2023-05-04 07:26:03.493276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-04 07:26:11 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='SRC', target_lang='TGT', trainpref='/content/finetuning/dataset_finetune/final/train', validpref='/content/finetuning/dataset_finetune/final/dev', testpref='/content/finetuning/dataset_finetune/final/test', align_suffix=None, destdir='/content/finetuning/dataset_finetune/final_bin', thresholdtgt=5, thresholdsrc=5, tgtdict='/content/finetuning/indic-en/final_bin/dict.TGT.txt', srcdict='/content/finetuning/indic-en/final_bin/dict.SRC.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=2, dict_only=False)\n",
            "2023-05-04 07:26:11 | INFO | fairseq_cli.preprocess | [SRC] Dictionary: 35904 types\n",
            "2023-05-04 07:26:12 | INFO | fairseq_cli.preprocess | [SRC] /content/finetuning/dataset_finetune/final/train.SRC: 5168 sents, 140490 tokens, 0.00285% replaced (by <unk>)\n",
            "2023-05-04 07:26:12 | INFO | fairseq_cli.preprocess | [SRC] Dictionary: 35904 types\n",
            "2023-05-04 07:26:13 | INFO | fairseq_cli.preprocess | [SRC] /content/finetuning/dataset_finetune/final/dev.SRC: 1000 sents, 27064 tokens, 0.0222% replaced (by <unk>)\n",
            "2023-05-04 07:26:13 | INFO | fairseq_cli.preprocess | [SRC] Dictionary: 35904 types\n",
            "2023-05-04 07:26:13 | INFO | fairseq_cli.preprocess | [SRC] /content/finetuning/dataset_finetune/final/test.SRC: 1000 sents, 27420 tokens, 0.00729% replaced (by <unk>)\n",
            "2023-05-04 07:26:13 | INFO | fairseq_cli.preprocess | [TGT] Dictionary: 32088 types\n",
            "2023-05-04 07:26:14 | INFO | fairseq_cli.preprocess | [TGT] /content/finetuning/dataset_finetune/final/train.TGT: 5168 sents, 115579 tokens, 0.242% replaced (by <unk>)\n",
            "2023-05-04 07:26:14 | INFO | fairseq_cli.preprocess | [TGT] Dictionary: 32088 types\n",
            "2023-05-04 07:26:14 | INFO | fairseq_cli.preprocess | [TGT] /content/finetuning/dataset_finetune/final/dev.TGT: 1000 sents, 22311 tokens, 0.17% replaced (by <unk>)\n",
            "2023-05-04 07:26:14 | INFO | fairseq_cli.preprocess | [TGT] Dictionary: 32088 types\n",
            "2023-05-04 07:26:14 | INFO | fairseq_cli.preprocess | [TGT] /content/finetuning/dataset_finetune/final/test.TGT: 1000 sents, 22240 tokens, 0.175% replaced (by <unk>)\n",
            "2023-05-04 07:26:14 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /content/finetuning/dataset_finetune/final_bin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "%%shell\n",
        "# Creating working directory and copying data into it:\n",
        "# exp_dir: Directory in which preprocessed data is stored\n",
        "# download_dir: Directory in which extracted en-indic model is stored\n",
        "# Copy the vocab from downloaded model to working directory\n",
        "\n",
        "exp_dir=/content/finetuning/dataset_finetune\n",
        "download_dir=/content/finetuning/indic-en\n",
        "src_lang=mr\n",
        "tgt_lang=en\n",
        "cp -r $download_dir/vocab $exp_dir\n",
        "\n",
        "echo \"Running experiment ${exp_dir} on ${src_lang} to ${tgt_lang}\"\n",
        "\n",
        "# Preparing directory structure:\n",
        "# train_processed_dir, devtest_processed_dir: For processed data\n",
        "# train_norm_dir, devtest_norm_dir: For Normalization output\n",
        "\n",
        "train_processed_dir=$exp_dir/data\n",
        "devtest_processed_dir=$exp_dir/data\n",
        "\n",
        "mkdir -p $train_processed_dir\n",
        "mkdir -p $devtest_processed_dir\n",
        "\n",
        "train_norm_dir=$exp_dir/norm/$src_lang-$tgt_lang\n",
        "devtest_norm_dir=$exp_dir/norm/$src_lang-$tgt_lang\n",
        "\n",
        "mkdir -p $train_norm_dir\n",
        "mkdir -p $devtest_norm_dir\n",
        "\n",
        "\n",
        "# Pre-processing of train, dev and test set\n",
        "\n",
        "datasets=(train dev test)\n",
        "\n",
        "for dataset in ${datasets[@]};do\n",
        "\t\tif [ $dataset == train ]; then\n",
        "\t\t\tin_dir=$exp_dir/$dataset/$src_lang-$tgt_lang\n",
        "\t\telse\n",
        "\t\t\tin_dir=$exp_dir/$dataset\n",
        "\t\tfi\n",
        "\n",
        "\t\top_dir=$exp_dir/norm/$src_lang-$tgt_lang\n",
        "\n",
        "    infname_src=$in_dir/$dataset.$src_lang\n",
        "    infname_tgt=$in_dir/$dataset.$tgt_lang\n",
        "\n",
        "\t\toutfname_src=$op_dir/$dataset.$src_lang\n",
        "    outfname_tgt=$op_dir/$dataset.$tgt_lang\n",
        "\n",
        "    echo \"Applying normalization and script conversion for $dataset $lang\"\n",
        "\n",
        "    input_size=`python scripts/preprocess_translate.py $infname_src $outfname_src $src_lang true`\n",
        "    input_size=`python scripts/preprocess_translate.py $infname_tgt $outfname_tgt $tgt_lang true`\n",
        "\n",
        "    echo \"Number of sentences in $dataset $lang: $input_size\"\n",
        "done\n",
        "\n",
        "#Concatenate Data\n",
        "python scripts/concat_joint_data.py $exp_dir/norm $exp_dir/data $src_lang $tgt_lang 'train'\n",
        "python scripts/concat_joint_data.py $exp_dir/norm $exp_dir/data $src_lang $tgt_lang 'dev'\n",
        "python scripts/concat_joint_data.py $exp_dir/norm $exp_dir/data $src_lang $tgt_lang 'test'\n",
        "\n",
        "#Apply Byte Pair Encoding (BPE):\n",
        "#BPE is a form of data compression algorithm in which the most common pair of consecutive bytes of data is replaced by the by the byte that has not occured in that data.\n",
        "echo \"Applying bpe to the new finetuning data\"\n",
        "bash apply_single_bpe_traindevtest_notag.sh $exp_dir\n",
        "\n",
        "#Create Final Prepared Data Dir from which Binarizer will take input, this dir will contain data with special language tags\n",
        "bin_input_data_dir=$exp_dir/final\n",
        "mkdir -p $bin_input_data_dir\n",
        "\n",
        "# Adding special language tags to indicate the source and the target language\n",
        "#Hence, to translate from English to Marathi, give input as _src_en_ _tgt_mr_\n",
        "\n",
        "echo \"Adding language tags\"\n",
        "python scripts/add_joint_tags_translate.py $exp_dir 'train'\n",
        "python scripts/add_joint_tags_translate.py $exp_dir 'dev'\n",
        "python scripts/add_joint_tags_translate.py $exp_dir 'test'\n",
        "\n",
        "#Preparing directory to store binarized output\n",
        "bin_out_data_dir=$exp_dir/final_bin\n",
        "\n",
        "rm -rf $bin_out_data_dir\n",
        "\n",
        "#To get number of cores processing simultaneously\n",
        "num_workers=`python -c \"import multiprocessing; print(multiprocessing.cpu_count())\"`\n",
        "\n",
        "#Binarization of data\n",
        "echo \"Binarizing data\"\n",
        "fairseq-preprocess --source-lang SRC --target-lang TGT \\\n",
        " --trainpref $bin_input_data_dir/train --validpref $bin_input_data_dir/dev --testpref $bin_input_data_dir/test \\\n",
        " --destdir $bin_out_data_dir --workers $num_workers \\\n",
        " --srcdict $download_dir/final_bin/dict.SRC.txt --tgtdict $download_dir/final_bin/dict.TGT.txt --thresholdtgt 5 --thresholdsrc 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k5hGK3ldw_F",
        "outputId": "30ae6def-0fc2-49ce-dc73-e14a9ed91c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-04 07:26:20.852311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-04 07:26:22 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-05-04 07:26:27 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '../dataset_finetune/tensorboard-wandb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'model_configs', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 256, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 256, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 1000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '../dataset_finetune/model', 'restore_file': '../en-indic/model/checkpoint_best.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 5, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='../dataset_finetune/tensorboard-wandb', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir='model_configs', empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=256, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=256, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_4x', max_epoch=0, max_update=1000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='../dataset_finetune/model', restore_file='../en-indic/model/checkpoint_best.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=5, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=5, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='../dataset_finetune/final_bin', source_lang='SRC', target_lang='TGT', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, max_source_positions=210, max_target_positions=210, dropout=0.2, no_seed_provided=False, encoder_embed_dim=1536, encoder_ffn_embed_dim=4096, encoder_attention_heads=16, encoder_normalize_before=False, decoder_embed_dim=1536, decoder_ffn_embed_dim=4096, decoder_attention_heads=16, encoder_embed_path=None, encoder_layers=6, encoder_learned_pos=False, decoder_embed_path=None, decoder_layers=6, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1536, decoder_input_dim=1536, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_4x'), 'task': {'_name': 'translation', 'data': '../dataset_finetune/final_bin', 'source_lang': 'SRC', 'target_lang': 'TGT', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 210, 'max_target_positions': 210, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-05-04 07:26:27 | INFO | fairseq.tasks.translation | [SRC] dictionary: 35904 types\n",
            "2023-05-04 07:26:27 | INFO | fairseq.tasks.translation | [TGT] dictionary: 32088 types\n",
            "2023-05-04 07:26:33 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(35904, 1536, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(32088, 1536, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=1536, out_features=32088, bias=False)\n",
            "  )\n",
            ")\n",
            "2023-05-04 07:26:33 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2023-05-04 07:26:33 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2023-05-04 07:26:33 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2023-05-04 07:26:33 | INFO | fairseq_cli.train | num. shared model params: 474,857,472 (num. trained: 474,857,472)\n",
            "2023-05-04 07:26:33 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-05-04 07:26:33 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: ../dataset_finetune/final_bin/valid.SRC-TGT.SRC\n",
            "2023-05-04 07:26:33 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: ../dataset_finetune/final_bin/valid.SRC-TGT.TGT\n",
            "2023-05-04 07:26:33 | INFO | fairseq.tasks.translation | ../dataset_finetune/final_bin valid SRC-TGT 1000 examples\n",
            "2023-05-04 07:26:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-05-04 07:26:40 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-05-04 07:26:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-05-04 07:26:40 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-05-04 07:26:40 | INFO | fairseq_cli.train | max tokens per device = 256 and max sentences per device = None\n",
            "2023-05-04 07:26:40 | INFO | fairseq.trainer | Preparing to load checkpoint ../en-indic/model/checkpoint_best.pt\n",
            "2023-05-04 07:26:40 | INFO | fairseq.trainer | No existing checkpoint found ../en-indic/model/checkpoint_best.pt\n",
            "2023-05-04 07:26:40 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-05-04 07:26:40 | INFO | fairseq.data.data_utils | loaded 5,168 examples from: ../dataset_finetune/final_bin/train.SRC-TGT.SRC\n",
            "2023-05-04 07:26:40 | INFO | fairseq.data.data_utils | loaded 5,168 examples from: ../dataset_finetune/final_bin/train.SRC-TGT.TGT\n",
            "2023-05-04 07:26:40 | INFO | fairseq.tasks.translation | ../dataset_finetune/final_bin train SRC-TGT 5168 examples\n",
            "2023-05-04 07:26:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-05-04 07:26:40 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2023-05-04 07:26:40 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2023-05-04 07:26:40 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2023-05-04 07:26:40 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-05-04 07:26:40 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
            "2023-05-04 07:26:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-05-04 07:26:40 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2023-05-04 07:26:40 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2023-05-04 07:26:40 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2023-05-04 07:26:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 351\n",
            "epoch 001:   0% 0/351 [00:00<?, ?it/s]2023-05-04 07:26:41 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-05-04 07:26:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 350/351 [02:51<00:00,  1.97it/s, loss=12.019, nll_loss=11.569, ppl=3038.9, wps=663.9, ups=2.02, wpb=328.6, bsz=14.5, num_updates=300, lr=2.3425e-06, gnorm=6.291, clip=100, train_wall=49, gb_free=7.2, wall=147]2023-05-04 07:29:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-05-04 07:29:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/138 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   1% 2/138 [00:00<00:10, 13.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 4/138 [00:00<00:08, 15.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 6/138 [00:00<00:07, 16.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 8/138 [00:00<00:07, 17.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   7% 10/138 [00:00<00:07, 17.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 13/138 [00:00<00:06, 18.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 16/138 [00:00<00:06, 19.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  13% 18/138 [00:00<00:06, 18.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 21/138 [00:01<00:05, 20.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 24/138 [00:01<00:05, 20.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 27/138 [00:01<00:05, 20.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 30/138 [00:01<00:05, 20.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 33/138 [00:01<00:04, 21.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 36/138 [00:01<00:04, 21.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  28% 39/138 [00:01<00:04, 21.50it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  30% 42/138 [00:02<00:04, 20.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 45/138 [00:02<00:04, 20.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 48/138 [00:02<00:04, 20.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  37% 51/138 [00:02<00:04, 19.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 53/138 [00:02<00:04, 19.67it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  40% 55/138 [00:02<00:04, 19.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41% 57/138 [00:02<00:04, 18.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 59/138 [00:03<00:04, 18.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 61/138 [00:03<00:04, 18.56it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 63/138 [00:03<00:04, 18.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 65/138 [00:03<00:04, 17.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  49% 67/138 [00:03<00:04, 17.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 69/138 [00:03<00:04, 16.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  51% 71/138 [00:03<00:03, 17.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 73/138 [00:03<00:03, 16.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  54% 75/138 [00:03<00:03, 16.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 77/138 [00:04<00:03, 16.43it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  57% 79/138 [00:04<00:03, 16.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 81/138 [00:04<00:03, 16.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 83/138 [00:04<00:03, 16.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 85/138 [00:04<00:03, 16.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  63% 87/138 [00:04<00:03, 16.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 89/138 [00:04<00:02, 16.43it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  66% 91/138 [00:04<00:02, 16.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 93/138 [00:05<00:02, 16.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  69% 95/138 [00:05<00:02, 16.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 97/138 [00:05<00:02, 16.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 99/138 [00:05<00:02, 16.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 101/138 [00:05<00:02, 16.50it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 103/138 [00:05<00:02, 16.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 105/138 [00:05<00:02, 16.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  78% 107/138 [00:05<00:01, 16.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 109/138 [00:06<00:01, 15.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 111/138 [00:06<00:01, 15.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 113/138 [00:06<00:01, 15.74it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 115/138 [00:06<00:01, 15.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 117/138 [00:06<00:01, 15.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86% 119/138 [00:06<00:01, 15.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 121/138 [00:06<00:01, 15.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 123/138 [00:06<00:00, 15.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 125/138 [00:07<00:00, 15.50it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 127/138 [00:07<00:00, 15.40it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 129/138 [00:07<00:00, 15.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 131/138 [00:07<00:00, 15.74it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 133/138 [00:07<00:00, 15.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 135/138 [00:07<00:00, 16.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  99% 137/138 [00:07<00:00, 16.67it/s]\u001b[A\n",
            "                                                                          \u001b[A2023-05-04 07:29:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.655 | nll_loss 11.133 | ppl 2246.25 | wps 2840.8 | wpb 161.1 | bsz 7.2 | num_updates 351\n",
            "2023-05-04 07:29:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 351 updates\n",
            "2023-05-04 07:29:41 | INFO | fairseq.trainer | Saving checkpoint to /content/finetuning/dataset_finetune/model/checkpoint1.pt\n",
            "2023-05-04 07:30:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/finetuning/dataset_finetune/model/checkpoint1.pt\n",
            "2023-05-04 07:31:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../dataset_finetune/model/checkpoint1.pt (epoch 1 @ 351 updates, score 11.655) (writing took 97.28609111900005 seconds)\n",
            "2023-05-04 07:31:18 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-05-04 07:31:18 | INFO | train | epoch 001 | loss 12.794 | nll_loss 12.442 | ppl 5565.08 | wps 420.2 | ups 1.28 | wpb 328.8 | bsz 14.7 | num_updates 351 | lr 2.72373e-06 | gnorm 8.267 | clip 100 | train_wall 169 | gb_free 7.4 | wall 278\n",
            "2023-05-04 07:31:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-05-04 07:31:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 351\n",
            "epoch 002:   0% 0/351 [00:00<?, ?it/s]2023-05-04 07:31:18 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-05-04 07:31:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 350/351 [02:57<00:00,  1.97it/s, loss=10.706, nll_loss=10.055, ppl=1063.84, wps=654.6, ups=1.96, wpb=334, bsz=14.6, num_updates=700, lr=5.3325e-06, gnorm=5.779, clip=100, train_wall=50, gb_free=7.2, wall=455]2023-05-04 07:34:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-05-04 07:34:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/138 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   1% 2/138 [00:00<00:08, 16.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   3% 4/138 [00:00<00:07, 16.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 6/138 [00:00<00:07, 17.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 8/138 [00:00<00:07, 16.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   7% 10/138 [00:00<00:07, 16.70it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 12/138 [00:00<00:07, 17.48it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 15/138 [00:00<00:06, 19.10it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 17/138 [00:00<00:06, 18.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  14% 20/138 [00:01<00:05, 19.71it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 23/138 [00:01<00:05, 20.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  19% 26/138 [00:01<00:05, 20.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 29/138 [00:01<00:05, 20.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 32/138 [00:01<00:05, 20.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 35/138 [00:01<00:04, 20.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  28% 38/138 [00:01<00:04, 20.68it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  30% 41/138 [00:02<00:04, 20.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 44/138 [00:02<00:04, 20.07it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  34% 47/138 [00:02<00:04, 19.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 49/138 [00:02<00:04, 19.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  37% 51/138 [00:02<00:04, 19.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 53/138 [00:02<00:04, 19.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  40% 55/138 [00:02<00:04, 18.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  41% 57/138 [00:02<00:04, 18.10it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  43% 59/138 [00:03<00:04, 17.67it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 61/138 [00:03<00:04, 17.79it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  46% 63/138 [00:03<00:04, 17.38it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 65/138 [00:03<00:04, 16.75it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  49% 67/138 [00:03<00:04, 16.64it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 69/138 [00:03<00:04, 16.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  51% 71/138 [00:03<00:04, 16.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 73/138 [00:03<00:03, 16.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  54% 75/138 [00:04<00:03, 16.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 77/138 [00:04<00:03, 15.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  57% 79/138 [00:04<00:03, 15.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  59% 81/138 [00:04<00:03, 16.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  60% 83/138 [00:04<00:03, 15.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 85/138 [00:04<00:03, 15.47it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  63% 87/138 [00:04<00:03, 15.35it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 89/138 [00:04<00:03, 15.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  66% 91/138 [00:05<00:02, 15.73it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 93/138 [00:05<00:02, 16.10it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  69% 95/138 [00:05<00:02, 16.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  70% 97/138 [00:05<00:02, 16.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  72% 99/138 [00:05<00:02, 15.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73% 101/138 [00:05<00:02, 15.85it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 103/138 [00:05<00:02, 15.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76% 105/138 [00:05<00:02, 15.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  78% 107/138 [00:06<00:01, 15.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 109/138 [00:06<00:01, 15.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80% 111/138 [00:06<00:01, 15.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 113/138 [00:06<00:01, 15.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 115/138 [00:06<00:01, 15.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 117/138 [00:06<00:01, 15.25it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  86% 119/138 [00:06<00:01, 15.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88% 121/138 [00:07<00:01, 14.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 123/138 [00:07<00:00, 15.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 125/138 [00:07<00:00, 14.90it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 127/138 [00:07<00:00, 15.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  93% 129/138 [00:07<00:00, 15.05it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 131/138 [00:07<00:00, 15.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 133/138 [00:07<00:00, 15.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98% 135/138 [00:07<00:00, 15.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  99% 137/138 [00:08<00:00, 15.56it/s]\u001b[A\n",
            "                                                                          \u001b[A2023-05-04 07:34:24 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.767 | nll_loss 10.064 | ppl 1070.2 | wps 2739.4 | wpb 161.1 | bsz 7.2 | num_updates 702 | best_loss 10.767\n",
            "2023-05-04 07:34:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 702 updates\n",
            "2023-05-04 07:34:24 | INFO | fairseq.trainer | Saving checkpoint to /content/finetuning/dataset_finetune/model/checkpoint2.pt\n",
            "2023-05-04 07:34:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/finetuning/dataset_finetune/model/checkpoint2.pt\n",
            "2023-05-04 07:35:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../dataset_finetune/model/checkpoint2.pt (epoch 2 @ 702 updates, score 10.767) (writing took 91.69304852300002 seconds)\n",
            "2023-05-04 07:35:56 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-05-04 07:35:56 | INFO | train | epoch 002 | loss 11.022 | nll_loss 10.429 | ppl 1378.93 | wps 414.9 | ups 1.26 | wpb 328.8 | bsz 14.7 | num_updates 702 | lr 5.34745e-06 | gnorm 6.225 | clip 100 | train_wall 176 | gb_free 7.4 | wall 556\n",
            "2023-05-04 07:35:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-05-04 07:35:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 351\n",
            "epoch 003:   0% 0/351 [00:00<?, ?it/s]2023-05-04 07:35:56 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-05-04 07:35:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  85% 297/351 [02:30<00:28,  1.90it/s, loss=10.187, nll_loss=9.428, ppl=689.01, wps=662.5, ups=2, wpb=330.9, bsz=15, num_updates=900, lr=6.8275e-06, gnorm=6.282, clip=100, train_wall=49, gb_free=7.3, wall=656]2023-05-04 07:38:28 | INFO | fairseq_cli.train | Stopping training due to num_updates: 1000 >= max_update: 1000\n",
            "2023-05-04 07:38:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-05-04 07:38:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/138 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   1% 1/138 [00:00<00:16,  8.09it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   2% 3/138 [00:00<00:09, 14.08it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   4% 5/138 [00:00<00:08, 15.38it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   5% 7/138 [00:00<00:08, 15.74it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   7% 9/138 [00:00<00:07, 16.45it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 11/138 [00:00<00:07, 16.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  10% 14/138 [00:00<00:06, 18.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  12% 17/138 [00:00<00:06, 18.81it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  14% 19/138 [00:01<00:06, 19.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  16% 22/138 [00:01<00:05, 19.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  18% 25/138 [00:01<00:05, 20.57it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 28/138 [00:01<00:05, 20.24it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  22% 31/138 [00:01<00:05, 20.49it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  25% 34/138 [00:01<00:05, 20.64it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  27% 37/138 [00:01<00:04, 20.57it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  29% 40/138 [00:02<00:04, 20.45it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  31% 43/138 [00:02<00:04, 20.11it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 46/138 [00:02<00:04, 19.84it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 49/138 [00:02<00:04, 19.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  37% 51/138 [00:02<00:04, 19.43it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  38% 53/138 [00:02<00:04, 19.01it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  40% 55/138 [00:02<00:04, 18.67it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  41% 57/138 [00:03<00:04, 18.43it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  43% 59/138 [00:03<00:04, 17.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 61/138 [00:03<00:04, 17.56it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  46% 63/138 [00:03<00:04, 17.46it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  47% 65/138 [00:03<00:04, 17.32it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  49% 67/138 [00:03<00:04, 17.21it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 69/138 [00:03<00:04, 16.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  51% 71/138 [00:03<00:03, 16.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  53% 73/138 [00:03<00:03, 16.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  54% 75/138 [00:04<00:03, 16.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  56% 77/138 [00:04<00:03, 16.15it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  57% 79/138 [00:04<00:03, 16.08it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  59% 81/138 [00:04<00:03, 16.11it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  60% 83/138 [00:04<00:03, 16.10it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  62% 85/138 [00:04<00:03, 15.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  63% 87/138 [00:04<00:03, 15.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 89/138 [00:04<00:03, 15.74it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  66% 91/138 [00:05<00:02, 16.14it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 93/138 [00:05<00:02, 16.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  69% 95/138 [00:05<00:02, 16.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  70% 97/138 [00:05<00:02, 16.29it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  72% 99/138 [00:05<00:02, 15.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  73% 101/138 [00:05<00:02, 16.43it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  75% 103/138 [00:05<00:02, 16.26it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  76% 105/138 [00:05<00:02, 16.29it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  78% 107/138 [00:06<00:01, 15.72it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  79% 109/138 [00:06<00:01, 16.09it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  80% 111/138 [00:06<00:01, 16.01it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  82% 113/138 [00:06<00:01, 15.78it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 115/138 [00:06<00:01, 15.78it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  85% 117/138 [00:06<00:01, 15.70it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  86% 119/138 [00:06<00:01, 15.30it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  88% 121/138 [00:07<00:01, 15.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 123/138 [00:07<00:00, 15.39it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  91% 125/138 [00:07<00:00, 15.26it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  92% 127/138 [00:07<00:00, 15.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  93% 129/138 [00:07<00:00, 15.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  95% 131/138 [00:07<00:00, 15.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  96% 133/138 [00:07<00:00, 15.39it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  98% 135/138 [00:07<00:00, 15.65it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  99% 137/138 [00:08<00:00, 16.16it/s]\u001b[A\n",
            "                                                                          \u001b[A2023-05-04 07:38:36 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.465 | nll_loss 9.648 | ppl 802.1 | wps 2779.5 | wpb 161.1 | bsz 7.2 | num_updates 1000 | best_loss 10.465\n",
            "2023-05-04 07:38:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1000 updates\n",
            "2023-05-04 07:38:36 | INFO | fairseq.trainer | Saving checkpoint to /content/finetuning/dataset_finetune/model/checkpoint_best.pt\n",
            "2023-05-04 07:39:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/finetuning/dataset_finetune/model/checkpoint_best.pt\n",
            "2023-05-04 07:39:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../dataset_finetune/model/checkpoint_best.pt (epoch 3 @ 1000 updates, score 10.465) (writing took 62.074144523000086 seconds)\n",
            "2023-05-04 07:39:38 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-05-04 07:39:38 | INFO | train | epoch 003 | loss 10.226 | nll_loss 9.473 | ppl 710.53 | wps 445.4 | ups 1.34 | wpb 331.3 | bsz 14.8 | num_updates 1000 | lr 7.575e-06 | gnorm 6.088 | clip 100 | train_wall 149 | gb_free 7.1 | wall 778\n",
            "2023-05-04 07:39:38 | INFO | fairseq_cli.train | done training in 776.9 seconds\n"
          ]
        }
      ],
      "source": [
        "!( fairseq-train ../dataset_finetune/final_bin \\\n",
        "--max-source-positions=210 \\\n",
        "--max-target-positions=210 \\\n",
        "--max-update=1000 \\\n",
        "--save-interval=1 \\\n",
        "--arch=transformer_4x \\\n",
        "--criterion=label_smoothed_cross_entropy \\\n",
        "--source-lang=SRC \\\n",
        "--lr-scheduler=inverse_sqrt \\\n",
        "--target-lang=TGT \\\n",
        "--label-smoothing=0.1 \\\n",
        "--optimizer adam \\\n",
        "--adam-betas \"(0.9, 0.98)\" \\\n",
        "--clip-norm 1.0 \\\n",
        "--warmup-init-lr 1e-07 \\\n",
        "--warmup-updates 4000 \\\n",
        "--dropout 0.2 \\\n",
        "--tensorboard-logdir ../dataset_finetune/tensorboard-wandb \\\n",
        "--save-dir ../dataset_finetune/model \\\n",
        "--keep-last-epochs 5 \\\n",
        "--patience 5 \\\n",
        "--skip-invalid-size-inputs-valid-test \\\n",
        "--user-dir model_configs \\\n",
        "--update-freq=2 \\\n",
        "--distributed-world-size 1 \\\n",
        "--max-tokens 256 \\\n",
        "--lr 3e-5 \\\n",
        "--restore-file ../en-indic/model/checkpoint_best.pt \\\n",
        "--reset-lr-scheduler \\\n",
        "--reset-meters \\\n",
        "--reset-dataloader \\\n",
        "--reset-optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AIRu2RtGczR",
        "outputId": "da812630-029a-455a-e97c-1211ceabf791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing vocab and bpe\n",
            "Initializing model for translation\n",
            "Initializing vocab and bpe\n",
            "Initializing model for translation\n"
          ]
        }
      ],
      "source": [
        "from finetuning.indicTrans.inference.engine import Model\n",
        "indic2en_model_base = Model(expdir='/content/finetuning/indic-en')\n",
        "indic2en_model_finetuned = Model(expdir='/content/finetuning/dataset_finetune')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54pauC9dl8fK"
      },
      "outputs": [],
      "source": [
        "f=open(\"/content/finetuning/dataset_finetune/test/test.en\",\"r\")\n",
        "test_en_list_full=[]\n",
        "for x in f:\n",
        "  test_en_list_full.append(x)\n",
        "\n",
        "test_en_list=test_en_list_full\n",
        "\n",
        "f=open(\"/content/finetuning/dataset_finetune/test/test.mr\",\"r\")\n",
        "test_mr_list_full=[]\n",
        "for x in f:\n",
        "  test_mr_list_full.append(x)\n",
        "\n",
        "test_mr_list=test_mr_list_full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HfcY1xNmCie",
        "outputId": "4cc04c6a-86a1-4d79-e625-65b2f19d52cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 2885.66it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2210.44it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4614.20it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 770.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5825.42it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2127.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5068.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4527.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5566.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4469.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4074.12it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4310.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4258.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2105.05it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4185.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1228.56it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4624.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6186.29it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3575.71it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1947.67it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1897.45it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1814.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2360.33it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 841.05it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2235.77it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 683.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4319.57it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1782.53it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 949.15it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1911.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1845.27it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 786.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1887.63it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2335.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2099.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1287.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1138.21it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6482.70it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3581.81it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3746.59it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 606.73it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4476.31it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4644.85it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2189.09it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3964.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1339.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4457.28it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4288.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6223.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1339.82it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5566.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3862.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3645.64it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3338.09it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1577.70it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4330.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4062.28it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4209.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4815.50it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4369.07it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4877.10it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4694.24it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4702.13it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3437.95it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5366.99it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6462.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2431.48it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2435.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2266.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2211.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1803.23it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1147.24it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 840.96it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2447.09it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2036.56it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1866.21it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2047.50it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2107.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2179.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1937.77it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2179.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2309.64it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2086.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2336.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1675.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4744.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3839.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6004.73it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2228.64it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4507.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4118.12it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3600.26it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5014.11it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4445.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4375.90it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4410.41it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4204.82it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4471.54it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1627.28it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4258.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3489.44it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1396.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1063.87it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3823.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 995.80it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3921.74it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 820.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 774.57it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4074.12it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4054.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4027.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3688.92it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4306.27it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4156.89it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 569.38it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1895.30it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2376.38it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2359.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1957.21it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 947.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1841.22it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2724.46it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1926.64it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1878.75it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1701.54it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3628.29it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4240.95it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4130.28it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2785.99it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2644.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3237.59it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2880.70it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3853.29it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3297.41it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4196.40it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4500.33it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4473.92it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4723.32it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3658.35it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3155.98it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1000.07it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3216.49it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3131.25it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4857.33it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2473.79it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2886.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3664.75it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3663.15it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4639.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2166.48it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1916.96it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1818.08it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2052.51it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2001.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1165.57it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1839.20it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1700.51it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 853.98it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1852.61it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1760.09it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2181.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3591.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3267.86it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3332.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3397.57it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3363.52it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1222.83it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3485.09it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5408.52it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3490.89it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3102.30it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3517.24it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3306.51it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2850.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3106.89it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3188.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2637.10it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2650.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3134.76it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3204.20it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3568.10it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3940.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1619.42it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1611.33it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1530.21it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1394.15it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3563.55it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1852.20it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1998.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 999.48it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2091.92it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1923.99it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3903.49it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3881.82it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3043.76it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3103.44it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3788.89it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3966.24it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3092.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5626.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2999.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2910.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3940.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4561.51it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4987.28it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4710.06it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3642.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5159.05it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1625.07it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4591.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1958.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3828.67it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1028.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3855.06it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5757.45it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4969.55it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3697.05it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3655.17it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3269.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3204.20it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1836.39it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2106.10it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1038.71it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1927.09it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1196.32it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 804.59it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1965.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2106.63it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1657.83it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3061.54it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3212.80it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3104.59it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3195.66it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3188.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2492.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2825.40it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2453.53it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 491.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3878.23it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3351.42it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3161.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3682.44it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5265.92it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4920.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3058.19it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3209.11it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4032.98it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2201.73it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1737.13it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 488.19it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2491.42it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2501.82it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 443.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 478.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2055.53it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1896.59it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2513.82it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2451.38it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2190.81it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2068.71it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2275.19it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2055.53it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4750.06it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2277.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5648.89it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1258.23it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3729.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3440.77it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4440.77it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4478.70it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4500.33it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4380.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4634.59it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4284.27it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3614.22it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3775.25it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3847.99it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3631.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4405.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4712.70it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4490.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3921.74it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4601.54it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4344.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3546.98it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3830.41it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3629.86it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2735.12it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6990.51it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4295.24it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6472.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3429.52it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1863.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1877.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1720.74it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1794.35it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2080.51it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1860.83it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2208.11it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2030.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2007.32it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1977.05it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5118.13it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2642.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2128.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4652.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 574.60it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1876.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3368.92it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1002.82it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2719.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1105.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1928.86it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6563.86it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4295.24it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4163.08it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4683.76it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4851.71it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1241.84it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5065.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1096.55it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1159.61it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1706.74it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4583.94it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4885.62it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6652.35it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6260.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3790.60it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4389.64it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3927.25it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4050.51it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3674.38it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3901.68it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3787.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3284.50it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3355.44it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2169.84it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2375.03it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1898.30it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2049.50it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1944.06it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2177.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2287.59it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2182.26it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2283.24it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2341.22it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2036.56it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1625.07it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2238.75it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 845.71it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2267.19it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2049.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3881.82it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3823.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5841.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4422.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 769.10it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4366.79it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4804.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4862.96it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1379.71it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 834.60it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1861.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3437.95it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4860.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1137.75it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6013.34it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2284.48it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4161.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 958.48it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6528.10it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4662.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4943.20it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6921.29it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6569.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1045.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4410.41it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4497.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5797.24it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4104.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2124.24it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4500.33it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2231.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2290.09it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2123.70it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1972.86it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2039.54it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2006.84it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1748.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1741.10it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2095.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2123.70it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2288.22it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2447.80it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1790.52it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1949.03it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2000.62it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1942.71it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4760.84it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3080.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2387.88it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3839.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1303.79it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3940.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4452.55it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4583.94it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3898.05it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 975.19it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3986.98it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4297.44it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 885.81it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1055.17it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3862.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 877.29it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3996.48it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1028.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3416.95it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3661.55it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3330.13it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 555.98it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3639.31it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1971.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2472.33it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2513.06it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1106.38it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1244.42it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2063.62it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1443.57it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5761.41it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4972.50it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2521.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2447.09it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2284.48it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2226.87it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2301.40it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1863.31it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1863.31it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2458.56it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2456.40it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1821.63it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2075.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2502.57it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2138.86it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1915.64it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5773.30it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3710.13it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3636.15it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4657.75it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6109.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4156.89it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4317.35it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2166.48it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3885.41it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4134.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3535.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4747.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4826.59it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5124.38it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1550.29it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3863.94it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3701.95it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3332.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1121.17it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5394.60it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4963.67it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4431.38it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4832.15it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5533.38it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2236.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4546.67it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4256.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 7163.63it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2229.23it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1708.82it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2923.88it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4290.85it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3802.63it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2177.73it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2164.24it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1802.06it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1831.57it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2276.42it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2373.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2084.13it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2034.10it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1721.80it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1666.39it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2405.68it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2705.13it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2735.12it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2487.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 986.31it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3698.68it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4561.51it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4497.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4739.33it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3097.71it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4438.42it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4171.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4084.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3617.34it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4527.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 988.29it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5426.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3330.13it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3617.34it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3583.34it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3166.71it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3073.88it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5213.55it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4445.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4108.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4829.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5450.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3422.52it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6311.97it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4000.29it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6737.84it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4742.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 864.27it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4019.46it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1164.11it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1230.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2278.27it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2182.83it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1748.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2211.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 536.29it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2359.66it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 533.63it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1762.68it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2150.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1901.32it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2378.40it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2343.19it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1971.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2505.56it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1877.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1046.22it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2208.11it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4546.67it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1083.80it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2446.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5919.98it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6105.25it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4126.22it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1320.83it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4539.29it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4905.62it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1241.29it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1220.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5262.61it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3575.71it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5537.03it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1972.40it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1252.22it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 538.39it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5656.51it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3493.80it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1637.12it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1878.75it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2753.07it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5210.32it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4353.20it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1978.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5130.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1639.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6631.31it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6260.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2269.03it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 622.81it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5181.35it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5178.15it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2144.88it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2167.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2176.60it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2156.45it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2317.94it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2003.97it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2444.95it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2253.19it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4934.48it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2102.94it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2197.70it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2048.50it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 713.74it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2352.39it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2452.81it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5184.55it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 521.39it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 975.99it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4578.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4681.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4522.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4578.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1193.94it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3919.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5996.15it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1906.50it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3595.63it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4134.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 597.27it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4332.96it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1136.51it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4215.38it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 905.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4232.40it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4481.09it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5181.35it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5895.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2932.05it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1026.51it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3880.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 913.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5133.79it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4373.62it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4874.26it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5302.53it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3292.23it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5405.03it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1647.73it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4056.39it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3851.52it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4616.74it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4531.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5041.23it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2049.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2116.73it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2269.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1649.68it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1585.15it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2100.83it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2250.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2455.68it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2182.83it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2525.17it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2504.81it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2229.23it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2238.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5429.52it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 610.26it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2128.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 495.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1798.97it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2102.41it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4739.33it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4652.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5551.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2432.19it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2618.98it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2576.35it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4217.50it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3968.12it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1756.41it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1130.54it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6770.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5805.27it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5391.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1043.88it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4790.75it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2240.55it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4098.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4544.21it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1804.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1781.40it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1985.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3744.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3603.35it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3404.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1996.81it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1864.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1642.25it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1945.41it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2157.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2086.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2340.57it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2292.60it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1787.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3591.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1165.41it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1008.61it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1061.71it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4549.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5837.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1964.09it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4230.26it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4310.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4056.39it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3918.08it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1821.63it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3606.45it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1010.07it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4188.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 901.23it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5071.71it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4144.57it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 773.79it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5140.08it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4874.26it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4790.75it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1355.63it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3901.68it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4102.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3811.27it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2744.96it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1720.39it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3821.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5737.76it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3958.76it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5289.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4882.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4877.10it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2070.75it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2241.15it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2125.31it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2411.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2245.95it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 962.11it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 970.90it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2660.52it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2262.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1911.28it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1784.05it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 687.76it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 326.96it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2199.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4897.03it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5380.76it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3669.56it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3653.57it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4122.17it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6335.81it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5996.15it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5486.34it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5289.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4502.74it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1320.62it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1591.46it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4068.19it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 635.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3247.62it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4438.42it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4497.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6264.83it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4192.21it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3164.32it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4118.12it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1171.92it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3146.51it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5108.77it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1419.87it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4161.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2033.60it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1837.19it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1795.51it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2143.23it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1920.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2451.38it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2312.82it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2592.28it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2316.66it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2347.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2408.44it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2149.27it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2328.88it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2506.31it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2059.56it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2353.71it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2188.52it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1140.07it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1134.67it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4578.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5155.87it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5111.89it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4464.40it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3389.34it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3202.98it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5090.17it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4860.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2296.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3126.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4476.31it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4707.41it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3724.96it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4321.80it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4914.24it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4566.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4497.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4185.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5391.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2410.52it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6260.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2326.94it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5614.86it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3821.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1121.77it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4215.38it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1410.09it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2293.22it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1361.35it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5194.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5121.25it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4471.54it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3947.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1294.54it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6326.25it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2594.68it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2081.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1872.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1972.40it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2118.87it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1992.54it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1923.99it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2030.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2086.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1962.71it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2224.50it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2043.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1816.11it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2423.75it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2298.25it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2110.87it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 814.59it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2351.07it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1660.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5698.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1421.08it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3693.79it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3617.34it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4860.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1110.63it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4993.22it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2165.92it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1728.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1037.30it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5233.07it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1118.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4846.11it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4925.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5124.38it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1393.23it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 902.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 908.64it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5691.05it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2242.94it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 8192.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2955.82it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 779.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4337.44it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3949.44it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1070.79it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1205.26it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 925.28it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1033.84it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2274.57it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 982.16it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1530.77it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2436.42it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4720.66it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4454.92it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4387.35it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2281.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2308.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1022.63it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2616.53it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2266.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4389.64it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2237.56it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 457.22it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2225.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2143.23it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1977.05it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 649.83it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4894.17it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4969.55it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4639.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4211.15it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5299.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4217.50it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4098.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6716.26it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2128.55it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2144.33it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 7695.97it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 7397.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1789.76it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1478.17it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6159.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6842.26it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5785.25it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6159.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2236.96it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6100.81it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5415.50it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 7358.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4230.26it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 990.86it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1392.07it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4879.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4154.83it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4678.53it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4922.89it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1126.59it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 712.35it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4782.56it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4054.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4387.35it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4052.47it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3990.77it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3996.48it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3919.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1990.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2102.94it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2933.08it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2744.96it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2155.35it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2064.63it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2432.89it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1951.75it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2367.66it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2754.88it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2122.09it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2034.10it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2117.27it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4987.28it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2190.24it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2332.11it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1860.83it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 961.89it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3456.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2253.19it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5127.51it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5886.74it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1760.46it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2552.06it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1472.46it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1113.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 943.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4335.20it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3990.77it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3200.54it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1843.25it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 881.80it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1362.23it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5353.29it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 831.96it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1530.21it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1229.28it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1286.60it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1330.26it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5339.66it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4744.69it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6181.73it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3092.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3483.64it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5272.54it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4249.55it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3898.05it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4410.41it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1155.30it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2193.10it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1986.88it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2140.50it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2338.61it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2114.60it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2293.22it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2309.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2070.24it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2244.74it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 477.85it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2173.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1991.12it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1762.31it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1653.25it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1950.84it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4578.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4632.03it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4616.74it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4681.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4068.19it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4549.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4823.81it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4660.34it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4642.28it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4185.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4308.48it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3736.57it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4862.96it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5360.13it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1176.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4330.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 6694.82it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 5370.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1188.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 898.23it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 1144.89it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 3736.57it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 4483.49it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 2072.28it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 2\n",
        "NUM_BATCHES = len(test_mr_list)//BATCH_SIZE + 1\n",
        "predictions_en_finetuned=[]\n",
        "predictions_en_base=[]\n",
        "for i in range(NUM_BATCHES):\n",
        "  mr_sents_batch = test_mr_list[BATCH_SIZE*i : BATCH_SIZE*(i+1)]\n",
        "  predictions_en_base.extend(indic2en_model_base.batch_translate(mr_sents_batch,'mr','en'))\n",
        "  predictions_en_finetuned.extend(indic2en_model_finetuned.batch_translate(mr_sents_batch,'mr','en'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PT6SJYPmL_K"
      },
      "outputs": [],
      "source": [
        "ref_en_list=[[x] for x in test_en_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5kIeDBxmRW2"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU\n",
        "bleu = BLEU()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuDElw7ARfeC",
        "outputId": "09192972-83ce-4b5c-f8e5-773007152cc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BLEU = 44.18 100.0/57.1/33.3/20.0 (BP = 1.000 ratio = 1.000 hyp_len = 8 ref_len = 8)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "bleu.corpus_score(predictions_en_base, ref_en_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qzd4s1bS3_B",
        "outputId": "dc0b9e8a-3d23-40c2-fef8-5464284c9bad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BLEU = 65.90 83.3/72.7/70.0/44.4 (BP = 1.000 ratio = 1.000 hyp_len = 12 ref_len = 12)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "bleu.corpus_score(predictions_en_finetuned, ref_en_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XfAPNjN821t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be347d5-d617-4168-8c50-19b0d7c7bb8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:00<00:00, 18844.75it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He said.',\n",
              " 'The Prime Minister said that the India.',\n",
              " 'The Prime Minister',\n",
              " 'The Prime Minister, the Prime Minister of the',\n",
              " 'The Prime Minister, the Prime Minister of the Prime Minister.',\n",
              " 'The Prime Minister, the Prime Minister.',\n",
              " 'The Prime Minister, the Prime Minister of the Prime Minister.',\n",
              " 'The Prime Minister',\n",
              " 'The Prime Minister, Shri Narendra Modi.',\n",
              " 'The Prime Minister, Shri Narendra Modi.',\n",
              " 'The Prime Minister',\n",
              " 'The Prime Minister',\n",
              " 'He said.',\n",
              " 'The Prime Minister, the']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "mr_sents=['चकावते ते सोने नसते','एक ना धड भाराभर चिंध्या','दुष्काळात तेरावा महिना','माकडाच्या हाती कोलीत','नाचता येईना अंगण वाकडे','अंथरूण पाहून पाय पसरावेत',\n",
        "          'उथळ पाण्याला खळखळाट फार','बळी तो कान पिळी','एका हाताने टाळी वाजत नसते','इकडे आड तिकडे विहीर','वरातीमागून घोडे','अतिपरिचयात अवज्ञा',\n",
        "          'वाहत्या गंगेत हात धुवून घेणे','बडा घर पोकळ वासा']\n",
        "indic2en_model_finetuned.batch_translate(mr_sents, 'mr', 'en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WY-wsVnCCBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ec6940-c987-40c5-8571-d07a528e1c3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:00<00:00, 13394.22it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Gold is not a metal',\n",
              " 'Not a single scratch',\n",
              " '13th month of drought',\n",
              " \"In the monkey's hand\",\n",
              " 'Unable to dance',\n",
              " 'Spread your feet by looking at the bed',\n",
              " 'Very deep water',\n",
              " 'The victim has a yellow ear',\n",
              " 'No clapping with one hand',\n",
              " 'A well here and there',\n",
              " 'Horses behind the chariot',\n",
              " 'Over-identity denial',\n",
              " 'Washing hands with soap',\n",
              " 'The big house']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "mr_sents=['चकावते ते सोने नसते','एक ना धड भाराभर चिंध्या','दुष्काळात तेरावा महिना','माकडाच्या हाती कोलीत','नाचता येईना अंगण वाकडे','अंथरूण पाहून पाय पसरावेत',\n",
        "          'उथळ पाण्याला खळखळाट फार','बळी तो कान पिळी','एका हाताने टाळी वाजत नसते','इकडे आड तिकडे विहीर','वरातीमागून घोडे','अतिपरिचयात अवज्ञा',\n",
        "          'वाहत्या गंगेत हात धुवून घेणे','बडा घर पोकळ वासा']\n",
        "indic2en_model_base.batch_translate(mr_sents, 'mr', 'en')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQL1v2QqgvyI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}