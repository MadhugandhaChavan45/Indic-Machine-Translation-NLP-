{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "####Fine-tuning en-indic model of indicTrans:"
      ],
      "metadata": {
        "id": "mdmaTWLA_R4z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W5-SBIxZepM",
        "outputId": "827efd9b-3130-4ce7-a33a-6a1c0bed7a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu117\n",
            "Collecting torch==1.13.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (1801.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m927.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp310-cp310-linux_x86_64.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.13.1\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchaudio-0.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu117) (4.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (1.22.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (1.26.15)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.1+cu118\n",
            "    Uninstalling torchvision-0.15.1+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.1+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.1+cu118\n",
            "    Uninstalling torchaudio-2.0.1+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1+cu117 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1+cu117 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.1+cu117 torchaudio-0.13.1+cu117 torchvision-0.14.1+cu117\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Exj0v-AzancI"
      },
      "outputs": [],
      "source": [
        "#Making a separate directory for keeping finetuning data and model\n",
        "\n",
        "!mkdir /content/finetuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/finetuning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6Yne1cdAMnS",
        "outputId": "4f5185cd-12d0-49e3-ad88-ddc3dc20920b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YpzLNQEaNx_",
        "outputId": "41b81682-525a-4e86-910d-c486c7cfd9c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indicTrans'...\n",
            "remote: Enumerating objects: 697, done.\u001b[K\n",
            "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
            "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
            "remote: Total 697 (delta 278), reused 344 (delta 240), pack-reused 297\u001b[K\n",
            "Receiving objects: 100% (697/697), 2.64 MiB | 9.82 MiB/s, done.\n",
            "Resolving deltas: 100% (405/405), done.\n",
            "/content/finetuning/indicTrans\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1362, done.\u001b[K\n",
            "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 1362 (delta 111), reused 98 (delta 93), pack-reused 1219\u001b[K\n",
            "Receiving objects: 100% (1362/1362), 9.56 MiB | 9.60 MiB/s, done.\n",
            "Resolving deltas: 100% (721/721), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 27.95 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Updating files: 100% (28/28), done.\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 597, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 597 (delta 8), reused 12 (delta 4), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (597/597), 252.23 KiB | 4.50 MiB/s, done.\n",
            "Resolving deltas: 100% (357/357), done.\n",
            "/content/finetuning\n"
          ]
        }
      ],
      "source": [
        "# clone the repo for running evaluation\n",
        "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
        "%cd indicTrans\n",
        "# clone requirements repositories\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zylyE9sUaTvT",
        "outputId": "f27bfc0a-3535-4948-b0a7-3abb27c6ab58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 43.0 kB of archives.\n",
            "After this operation, 115 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tree amd64 1.8.0-1 [43.0 kB]\n",
            "Fetched 43.0 kB in 0s (139 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.8.0-1_amd64.deb ...\n",
            "Unpacking tree (1.8.0-1) ...\n",
            "Setting up tree (1.8.0-1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting mock\n",
            "  Downloading mock-5.0.2-py3-none-any.whl (30 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (9.0.0)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.91-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Collecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Collecting sphinx-argparse\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme\n",
            "  Downloading sphinx_rtd_theme-1.2.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (3.5.4)\n",
            "Collecting sphinxcontrib-jquery!=3.0.0,>=2.0.0\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: docutils<0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.16)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.13)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.3)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.12.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (67.7.2)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.27.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.12)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=8e9a6f055a3254026446212eba8c55d1096b9a29eeff757ead5d7ffde3917c66\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: morfessor, tensorboardX, sacremoses, portalocker, mock, colorama, sacrebleu, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed colorama-0.4.6 indic-nlp-library-0.91 mock-5.0.2 morfessor-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1 sacremoses-0.0.53 sphinx-argparse-0.4.0 sphinx-rtd-theme-1.2.0 sphinxcontrib-jquery-4.1 tensorboardX-2.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mosestokenizer\n",
            "  Downloading mosestokenizer-1.2.1.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openfile\n",
            "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
            "Collecting uctools\n",
            "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting toolwrapper\n",
            "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from subword-nmt) (4.65.0)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.10/dist-packages (from subword-nmt) (5.0.2)\n",
            "Building wheels for collected packages: mosestokenizer, docopt, toolwrapper, uctools\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mosestokenizer: filename=mosestokenizer-1.2.1-py3-none-any.whl size=49186 sha256=fc2e6cb2b249c916449eeaf4d7d296fb3fdf0d54d3d409df5743edc3e92fc073\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d8/15/4c5ebbe883513f003cb055a0369c77c9df857023a706f39e70\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=4a93889ba69aae1f054dfb36c3e7dff097c59f3840d8cca99205f51720c8a180\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3350 sha256=8fdf077dd5594632196a6f92c245a6009e6d3e96011248094c4f43163ebba8d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/af/b1/99b57a06dda78fdcee86d2e22c64743f3b8df8f31cfc04baf7\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6160 sha256=666c22dd80fa6dafaec4f923d8b82ce1633ca0b0b1e19a0d2d94bf77cb6f31df\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/ee/10/33257b0801ac6a912c841939032c16da1eb3db377afe1443e5\n",
            "Successfully built mosestokenizer docopt toolwrapper uctools\n",
            "Installing collected packages: toolwrapper, openfile, docopt, uctools, subword-nmt, mosestokenizer\n",
            "Successfully installed docopt-0.6.2 mosestokenizer-1.2.1 openfile-0.0.7 subword-nmt-0.3.8 toolwrapper-2.1.0 uctools-1.3.0\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 34544, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 34544 (delta 0), reused 1 (delta 0), pack-reused 34543\u001b[K\n",
            "Receiving objects: 100% (34544/34544), 24.05 MiB | 24.77 MiB/s, done.\n",
            "Resolving deltas: 100% (25092/25092), done.\n",
            "/content/finetuning/fairseq\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/finetuning/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.7/272.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.15.1)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (0.13.1+cu117)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (23.1)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.65.0)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.2.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2022.10.31)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (0.29.34)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.3.1)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.13.1+cu117)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.7.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.10.1)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=21479697 sha256=a87a0dd9bdc8590333bdd47879ba9e49895cec13c15279cd27ab7186d5cadef1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e7h4kx38/wheels/31/09/62/01f91bc6958b01e6f9867ecf5e28929a376a12e25ce4a134fe\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=ea7400cb1759247f9724dae65c653c56d520fb6bf08b65c90c853b4abc8deb0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, omegaconf, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.7.3 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.19-cp310-cp310-manylinux2014_x86_64.whl (108.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.2/108.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==2.0.0\n",
            "  Using cached torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "Collecting pyre-extensions==0.0.29\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (4.5.0)\n",
            "Collecting typing-inspect\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3\n",
            "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58\n",
            "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
            "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91\n",
            "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (1.11.1)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
            "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
            "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.12.0)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91\n",
            "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->xformers) (0.40.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->xformers) (67.7.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->xformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->xformers) (16.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->xformers) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->xformers) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, mypy-extensions, typing-inspect, nvidia-cusolver-cu11, nvidia-cudnn-cu11, pyre-extensions, torch, xformers\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu117\n",
            "    Uninstalling torch-1.13.1+cu117:\n",
            "      Successfully uninstalled torch-1.13.1+cu117\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu117 requires torch==1.13.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchaudio 0.13.1+cu117 requires torch==1.13.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mypy-extensions-1.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pyre-extensions-0.0.29 torch-2.0.0 typing-inspect-0.8.0 xformers-0.0.19\n",
            "/content/finetuning\n"
          ]
        }
      ],
      "source": [
        "! sudo apt install tree\n",
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "! pip install mosestokenizer subword-nmt\n",
        "# Install fairseq from source\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "%cd fairseq\n",
        "# !git checkout da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d\n",
        "!pip install ./\n",
        "! pip install xformers\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XkB4yRHaWpe"
      },
      "outputs": [],
      "source": [
        "# add fairseq folder to python path\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O942YOwwa34T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a0821b-a128-41e0-c3db-f68a70a523ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-04 06:22:48--  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
            "Resolving ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)... 164.52.210.97, 101.53.152.33, 101.53.152.30, ...\n",
            "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|164.52.210.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4811880516 (4.5G) [application/zip]\n",
            "Saving to: ‘en2indic.zip’\n",
            "\n",
            "en2indic.zip        100%[===================>]   4.48G  18.1MB/s    in 4m 39s  \n",
            "\n",
            "2023-05-04 06:27:29 (16.4 MB/s) - ‘en2indic.zip’ saved [4811880516/4811880516]\n",
            "\n",
            "Archive:  en2indic.zip\n",
            "   creating: en-indic/\n",
            "   creating: en-indic/vocab/\n",
            "  inflating: en-indic/vocab/bpe_codes.32k.SRC  \n",
            "  inflating: en-indic/vocab/vocab.SRC  \n",
            "  inflating: en-indic/vocab/vocab.TGT  \n",
            "  inflating: en-indic/vocab/bpe_codes.32k.TGT  \n",
            "   creating: en-indic/final_bin/\n",
            "  inflating: en-indic/final_bin/preprocess.log  \n",
            "  inflating: en-indic/final_bin/dict.TGT.txt  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.SRC.idx  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.TGT.idx  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.SRC.idx  \n",
            "  inflating: en-indic/final_bin/dict.SRC.txt  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.TGT.idx  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.TGT.bin  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.TGT.bin  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.TGT.idx  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.TGT.bin  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.SRC.idx  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.SRC.bin  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.SRC.bin  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.SRC.bin  \n",
            "   creating: en-indic/model/\n",
            "  inflating: en-indic/model/checkpoint_best.pt  \n"
          ]
        }
      ],
      "source": [
        "# downloading the en-indic model\n",
        "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
        "!unzip en2indic.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm-PEHOzaa6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d412b69-ce02-456a-c079-dd7ae75c4152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetuning\n",
            "--2023-05-04 06:28:45--  https://drive.google.com/uc?id=1YFzRxmOe9BVCcE0LRZvPiefrNiqVkY7Z&export=downloa\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.96.102, 108.177.96.138, 108.177.96.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.96.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-a4-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/r3cv0dvgkj5eqn30td14bc4t8d00ql1r/1683181725000/13340322011215046102/*/1YFzRxmOe9BVCcE0LRZvPiefrNiqVkY7Z?e=downloa&uuid=96f4b385-9f51-4c62-9323-7f6b9ebdcbbb [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-05-04 06:28:47--  https://doc-08-a4-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/r3cv0dvgkj5eqn30td14bc4t8d00ql1r/1683181725000/13340322011215046102/*/1YFzRxmOe9BVCcE0LRZvPiefrNiqVkY7Z?e=downloa&uuid=96f4b385-9f51-4c62-9323-7f6b9ebdcbbb\n",
            "Resolving doc-08-a4-docs.googleusercontent.com (doc-08-a4-docs.googleusercontent.com)... 108.177.127.132, 2a00:1450:4013:c07::84\n",
            "Connecting to doc-08-a4-docs.googleusercontent.com (doc-08-a4-docs.googleusercontent.com)|108.177.127.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1283558 (1.2M) [application/x-zip-compressed]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>]   1.22M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2023-05-04 06:28:47 (130 MB/s) - ‘dataset.zip’ saved [1283558/1283558]\n",
            "\n",
            "Archive:  dataset.zip\n",
            "   creating: dataset_finetune/dev/\n",
            "  inflating: dataset_finetune/dev/dev.en  \n",
            "  inflating: dataset_finetune/dev/dev.mr  \n",
            "   creating: dataset_finetune/test/\n",
            "  inflating: dataset_finetune/test/test.en  \n",
            "  inflating: dataset_finetune/test/test.mr  \n",
            "   creating: dataset_finetune/train/\n",
            "   creating: dataset_finetune/train/en-mr/\n",
            "  inflating: dataset_finetune/train/en-mr/train.en  \n",
            "  inflating: dataset_finetune/train/en-mr/train.mr  \n",
            "   creating: dataset_finetune/train/mr-en/\n",
            "  inflating: dataset_finetune/train/mr-en/train.en  \n",
            "  inflating: dataset_finetune/train/mr-en/train.mr  \n",
            "/content/finetuning/indicTrans\n"
          ]
        }
      ],
      "source": [
        "%cd /content/finetuning\n",
        "!wget \"https://drive.google.com/uc?id=1YFzRxmOe9BVCcE0LRZvPiefrNiqVkY7Z&export=download\" -O dataset.zip\n",
        "!unzip dataset.zip\n",
        "%cd /content/finetuning/indicTrans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCNDq3szds5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d57743-d8b0-4508-a1c7-19cacfc8a615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment /content/finetuning/dataset_finetune on en to mr\n",
            "Applying normalization and script conversion for train \n",
            "100% 5168/5168 [00:01<00:00, 4458.91it/s]\n",
            "100% 5168/5168 [00:00<00:00, 9831.83it/s]\n",
            "Number of sentences in train : 5168\n",
            "Applying normalization and script conversion for dev \n",
            "100% 1000/1000 [00:00<00:00, 2111.34it/s]\n",
            "100% 1000/1000 [00:00<00:00, 5436.85it/s]\n",
            "Number of sentences in dev : 1000\n",
            "Applying normalization and script conversion for test \n",
            "100% 1000/1000 [00:00<00:00, 2231.15it/s]\n",
            "100% 1000/1000 [00:00<00:00, 6240.39it/s]\n",
            "Number of sentences in test : 1000\n",
            "\n",
            "/content/finetuning/dataset_finetune/data/train.SRC\n",
            "/content/finetuning/dataset_finetune/data/train.TGT\n",
            "  0% 0/11 [00:00<?, ?it/s]src: en, tgt:as\n",
            "src: en, tgt:bn\n",
            "src: en, tgt:gu\n",
            "src: en, tgt:hi\n",
            "src: en, tgt:kn\n",
            "src: en, tgt:ml\n",
            "src: en, tgt:mr\n",
            "/content/finetuning/dataset_finetune/norm/en-mr/train.en\n",
            "/content/finetuning/dataset_finetune/norm/en-mr/train.mr\n",
            "src: en, tgt:or\n",
            "src: en, tgt:pa\n",
            "src: en, tgt:ta\n",
            "src: en, tgt:te\n",
            "100% 11/11 [00:00<00:00, 1238.09it/s]\n",
            "  0% 0/11 [00:00<?, ?it/s]src: en, tgt:as\n",
            "src: en, tgt:bn\n",
            "src: en, tgt:gu\n",
            "src: en, tgt:hi\n",
            "src: en, tgt:kn\n",
            "src: en, tgt:ml\n",
            "src: en, tgt:mr\n",
            "/content/finetuning/dataset_finetune/norm/en-mr/train.en\n",
            "src: en, tgt:or\n",
            "src: en, tgt:pa\n",
            "src: en, tgt:ta\n",
            "src: en, tgt:te\n",
            "100% 11/11 [00:00<00:00, 7913.78it/s]\n",
            "\n",
            "/content/finetuning/dataset_finetune/data/dev.SRC\n",
            "/content/finetuning/dataset_finetune/data/dev.TGT\n",
            "  0% 0/11 [00:00<?, ?it/s]src: en, tgt:as\n",
            "src: en, tgt:bn\n",
            "src: en, tgt:gu\n",
            "src: en, tgt:hi\n",
            "src: en, tgt:kn\n",
            "src: en, tgt:ml\n",
            "src: en, tgt:mr\n",
            "/content/finetuning/dataset_finetune/norm/en-mr/dev.en\n",
            "/content/finetuning/dataset_finetune/norm/en-mr/dev.mr\n",
            "src: en, tgt:or\n",
            "src: en, tgt:pa\n",
            "src: en, tgt:ta\n",
            "src: en, tgt:te\n",
            "100% 11/11 [00:00<00:00, 2970.85it/s]\n",
            "  0% 0/11 [00:00<?, ?it/s]src: en, tgt:as\n",
            "src: en, tgt:bn\n",
            "src: en, tgt:gu\n",
            "src: en, tgt:hi\n",
            "src: en, tgt:kn\n",
            "src: en, tgt:ml\n",
            "src: en, tgt:mr\n",
            "/content/finetuning/dataset_finetune/norm/en-mr/dev.en\n",
            "src: en, tgt:or\n",
            "src: en, tgt:pa\n",
            "src: en, tgt:ta\n",
            "src: en, tgt:te\n",
            "100% 11/11 [00:00<00:00, 20016.20it/s]\n",
            "\n",
            "/content/finetuning/dataset_finetune/data/test.SRC\n",
            "/content/finetuning/dataset_finetune/data/test.TGT\n",
            "  0% 0/11 [00:00<?, ?it/s]src: en, tgt:as\n",
            "src: en, tgt:bn\n",
            "src: en, tgt:gu\n",
            "src: en, tgt:hi\n",
            "src: en, tgt:kn\n",
            "src: en, tgt:ml\n",
            "src: en, tgt:mr\n",
            "/content/finetuning/dataset_finetune/norm/en-mr/test.en\n",
            "/content/finetuning/dataset_finetune/norm/en-mr/test.mr\n",
            "src: en, tgt:or\n",
            "src: en, tgt:pa\n",
            "src: en, tgt:ta\n",
            "src: en, tgt:te\n",
            "100% 11/11 [00:00<00:00, 3215.37it/s]\n",
            "  0% 0/11 [00:00<?, ?it/s]src: en, tgt:as\n",
            "src: en, tgt:bn\n",
            "src: en, tgt:gu\n",
            "src: en, tgt:hi\n",
            "src: en, tgt:kn\n",
            "src: en, tgt:ml\n",
            "src: en, tgt:mr\n",
            "/content/finetuning/dataset_finetune/norm/en-mr/test.en\n",
            "src: en, tgt:or\n",
            "src: en, tgt:pa\n",
            "src: en, tgt:ta\n",
            "src: en, tgt:te\n",
            "100% 11/11 [00:00<00:00, 22528.00it/s]\n",
            "Applying bpe to the new finetuning data\n",
            "train\n",
            "Apply to SRC corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "dev\n",
            "Apply to SRC corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "test\n",
            "Apply to SRC corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:444: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Adding language tags\n",
            "5168it [00:00, 143862.29it/s]\n",
            "1000it [00:00, 120929.07it/s]\n",
            "1000it [00:00, 144139.11it/s]\n",
            "Binarizing data\n",
            "2023-05-04 06:29:08.231398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-04 06:29:14 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='SRC', target_lang='TGT', trainpref='/content/finetuning/dataset_finetune/final/train', validpref='/content/finetuning/dataset_finetune/final/dev', testpref='/content/finetuning/dataset_finetune/final/test', align_suffix=None, destdir='/content/finetuning/dataset_finetune/final_bin', thresholdtgt=5, thresholdsrc=5, tgtdict='/content/finetuning/en-indic/final_bin/dict.TGT.txt', srcdict='/content/finetuning/en-indic/final_bin/dict.SRC.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=2, dict_only=False)\n",
            "2023-05-04 06:29:14 | INFO | fairseq_cli.preprocess | [SRC] Dictionary: 32104 types\n",
            "2023-05-04 06:29:16 | INFO | fairseq_cli.preprocess | [SRC] /content/finetuning/dataset_finetune/final/train.SRC: 5168 sents, 125759 tokens, 0.223% replaced (by <unk>)\n",
            "2023-05-04 06:29:16 | INFO | fairseq_cli.preprocess | [SRC] Dictionary: 32104 types\n",
            "2023-05-04 06:29:16 | INFO | fairseq_cli.preprocess | [SRC] /content/finetuning/dataset_finetune/final/dev.SRC: 1000 sents, 24311 tokens, 0.156% replaced (by <unk>)\n",
            "2023-05-04 06:29:16 | INFO | fairseq_cli.preprocess | [SRC] Dictionary: 32104 types\n",
            "2023-05-04 06:29:16 | INFO | fairseq_cli.preprocess | [SRC] /content/finetuning/dataset_finetune/final/test.SRC: 1000 sents, 24240 tokens, 0.161% replaced (by <unk>)\n",
            "2023-05-04 06:29:16 | INFO | fairseq_cli.preprocess | [TGT] Dictionary: 35888 types\n",
            "2023-05-04 06:29:18 | INFO | fairseq_cli.preprocess | [TGT] /content/finetuning/dataset_finetune/final/train.TGT: 5168 sents, 129998 tokens, 0.00308% replaced (by <unk>)\n",
            "2023-05-04 06:29:18 | INFO | fairseq_cli.preprocess | [TGT] Dictionary: 35888 types\n",
            "2023-05-04 06:29:19 | INFO | fairseq_cli.preprocess | [TGT] /content/finetuning/dataset_finetune/final/dev.TGT: 1000 sents, 25064 tokens, 0.0239% replaced (by <unk>)\n",
            "2023-05-04 06:29:19 | INFO | fairseq_cli.preprocess | [TGT] Dictionary: 35888 types\n",
            "2023-05-04 06:29:19 | INFO | fairseq_cli.preprocess | [TGT] /content/finetuning/dataset_finetune/final/test.TGT: 1000 sents, 25420 tokens, 0.00787% replaced (by <unk>)\n",
            "2023-05-04 06:29:19 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /content/finetuning/dataset_finetune/final_bin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "%%shell\n",
        "# Creating working directory and copying data into it:\n",
        "# exp_dir: Directory in which preprocessed data is stored\n",
        "# download_dir: Directory in which extracted en-indic model is stored\n",
        "# Copy the vocab from downloaded model to working directory\n",
        "\n",
        "exp_dir=/content/finetuning/dataset_finetune\n",
        "download_dir=/content/finetuning/en-indic\n",
        "src_lang=en\n",
        "tgt_lang=mr\n",
        "cp -r $download_dir/vocab $exp_dir\n",
        "\n",
        "echo \"Running experiment ${exp_dir} on ${src_lang} to ${tgt_lang}\"\n",
        "\n",
        "# Preparing directory structure:\n",
        "# train_processed_dir, devtest_processed_dir: For processed data\n",
        "# train_norm_dir, devtest_norm_dir: For Normalization output\n",
        "\n",
        "train_processed_dir=$exp_dir/data\n",
        "devtest_processed_dir=$exp_dir/data\n",
        "\n",
        "mkdir -p $train_processed_dir\n",
        "mkdir -p $devtest_processed_dir\n",
        "\n",
        "train_norm_dir=$exp_dir/norm/$src_lang-$tgt_lang\n",
        "devtest_norm_dir=$exp_dir/norm/$src_lang-$tgt_lang\n",
        "\n",
        "mkdir -p $train_norm_dir\n",
        "mkdir -p $devtest_norm_dir\n",
        "\n",
        "\n",
        "# Pre-processing of train, dev and test set\n",
        "\n",
        "datasets=(train dev test)\n",
        "\n",
        "for dataset in ${datasets[@]};do\n",
        "\t\tif [ $dataset == train ]; then\n",
        "\t\t\tin_dir=$exp_dir/$dataset/$src_lang-$tgt_lang\n",
        "\t\telse\n",
        "\t\t\tin_dir=$exp_dir/$dataset\n",
        "\t\tfi\n",
        "\n",
        "\t\top_dir=$exp_dir/norm/$src_lang-$tgt_lang\n",
        "\n",
        "    infname_src=$in_dir/$dataset.$src_lang\n",
        "    infname_tgt=$in_dir/$dataset.$tgt_lang\n",
        "\n",
        "\t\toutfname_src=$op_dir/$dataset.$src_lang\n",
        "    outfname_tgt=$op_dir/$dataset.$tgt_lang\n",
        "\n",
        "    echo \"Applying normalization and script conversion for $dataset $lang\"\n",
        "\n",
        "    input_size=`python scripts/preprocess_translate.py $infname_src $outfname_src $src_lang true`\n",
        "    input_size=`python scripts/preprocess_translate.py $infname_tgt $outfname_tgt $tgt_lang true`\n",
        "\n",
        "    echo \"Number of sentences in $dataset $lang: $input_size\"\n",
        "done\n",
        "\n",
        "#Concatenate Data\n",
        "python scripts/concat_joint_data.py $exp_dir/norm $exp_dir/data $src_lang $tgt_lang 'train'\n",
        "python scripts/concat_joint_data.py $exp_dir/norm $exp_dir/data $src_lang $tgt_lang 'dev'\n",
        "python scripts/concat_joint_data.py $exp_dir/norm $exp_dir/data $src_lang $tgt_lang 'test'\n",
        "\n",
        "#Apply Byte Pair Encoding (BPE):\n",
        "#BPE is a form of data compression algorithm in which the most common pair of consecutive bytes of data is replaced by the by the byte that has not occured in that data.\n",
        "echo \"Applying bpe to the new finetuning data\"\n",
        "bash apply_single_bpe_traindevtest_notag.sh $exp_dir\n",
        "\n",
        "#Create Final Prepared Data Dir from which Binarizer will take input, this dir will contain data with special language tags\n",
        "bin_input_data_dir=$exp_dir/final\n",
        "mkdir -p $bin_input_data_dir\n",
        "\n",
        "# Adding special language tags to indicate the source and the target language\n",
        "#Hence, to translate from English to Marathi, give input as _src_en_ _tgt_mr_\n",
        "\n",
        "echo \"Adding language tags\"\n",
        "python scripts/add_joint_tags_translate.py $exp_dir 'train'\n",
        "python scripts/add_joint_tags_translate.py $exp_dir 'dev'\n",
        "python scripts/add_joint_tags_translate.py $exp_dir 'test'\n",
        "\n",
        "#Preparing directory to store binarized output\n",
        "bin_out_data_dir=$exp_dir/final_bin\n",
        "\n",
        "rm -rf $bin_out_data_dir\n",
        "\n",
        "#To get number of cores processing simultaneously\n",
        "num_workers=`python -c \"import multiprocessing; print(multiprocessing.cpu_count())\"`\n",
        "\n",
        "#Binarization of data\n",
        "echo \"Binarizing data\"\n",
        "fairseq-preprocess --source-lang SRC --target-lang TGT \\\n",
        " --trainpref $bin_input_data_dir/train --validpref $bin_input_data_dir/dev --testpref $bin_input_data_dir/test \\\n",
        " --destdir $bin_out_data_dir --workers $num_workers \\\n",
        " --srcdict $download_dir/final_bin/dict.SRC.txt --tgtdict $download_dir/final_bin/dict.TGT.txt --thresholdtgt 5 --thresholdsrc 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k5hGK3ldw_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03803a4a-032a-409c-863a-957c683d60c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-04 06:29:25.361415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-04 06:29:26 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-05-04 06:29:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '../dataset_finetune/tensorboard-wandb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'model_configs', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 256, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 256, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 1000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '../dataset_finetune/model', 'restore_file': '../en-indic/model/checkpoint_best.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 5, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='../dataset_finetune/tensorboard-wandb', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir='model_configs', empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=256, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=256, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_4x', max_epoch=0, max_update=1000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='../dataset_finetune/model', restore_file='../en-indic/model/checkpoint_best.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=5, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=5, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='../dataset_finetune/final_bin', source_lang='SRC', target_lang='TGT', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, max_source_positions=210, max_target_positions=210, dropout=0.2, no_seed_provided=False, encoder_embed_dim=1536, encoder_ffn_embed_dim=4096, encoder_attention_heads=16, encoder_normalize_before=False, decoder_embed_dim=1536, decoder_ffn_embed_dim=4096, decoder_attention_heads=16, encoder_embed_path=None, encoder_layers=6, encoder_learned_pos=False, decoder_embed_path=None, decoder_layers=6, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1536, decoder_input_dim=1536, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_4x'), 'task': {'_name': 'translation', 'data': '../dataset_finetune/final_bin', 'source_lang': 'SRC', 'target_lang': 'TGT', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 210, 'max_target_positions': 210, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-05-04 06:29:30 | INFO | fairseq.tasks.translation | [SRC] dictionary: 32104 types\n",
            "2023-05-04 06:29:30 | INFO | fairseq.tasks.translation | [TGT] dictionary: 35888 types\n",
            "2023-05-04 06:29:39 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(32104, 1536, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(35888, 1536, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=1536, out_features=35888, bias=False)\n",
            "  )\n",
            ")\n",
            "2023-05-04 06:29:39 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2023-05-04 06:29:39 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2023-05-04 06:29:39 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2023-05-04 06:29:39 | INFO | fairseq_cli.train | num. shared model params: 480,694,272 (num. trained: 480,694,272)\n",
            "2023-05-04 06:29:39 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-05-04 06:29:39 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: ../dataset_finetune/final_bin/valid.SRC-TGT.SRC\n",
            "2023-05-04 06:29:39 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: ../dataset_finetune/final_bin/valid.SRC-TGT.TGT\n",
            "2023-05-04 06:29:39 | INFO | fairseq.tasks.translation | ../dataset_finetune/final_bin valid SRC-TGT 1000 examples\n",
            "2023-05-04 06:29:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-05-04 06:29:45 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-05-04 06:29:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-05-04 06:29:45 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-05-04 06:29:45 | INFO | fairseq_cli.train | max tokens per device = 256 and max sentences per device = None\n",
            "2023-05-04 06:29:45 | INFO | fairseq.trainer | Preparing to load checkpoint ../en-indic/model/checkpoint_best.pt\n",
            "2023-05-04 06:30:09 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-05-04 06:30:09 | INFO | fairseq.trainer | Loaded checkpoint ../en-indic/model/checkpoint_best.pt (epoch 4 @ 0 updates)\n",
            "2023-05-04 06:30:09 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-05-04 06:30:09 | INFO | fairseq.data.data_utils | loaded 5,168 examples from: ../dataset_finetune/final_bin/train.SRC-TGT.SRC\n",
            "2023-05-04 06:30:09 | INFO | fairseq.data.data_utils | loaded 5,168 examples from: ../dataset_finetune/final_bin/train.SRC-TGT.TGT\n",
            "2023-05-04 06:30:09 | INFO | fairseq.tasks.translation | ../dataset_finetune/final_bin train SRC-TGT 5168 examples\n",
            "2023-05-04 06:30:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-05-04 06:30:09 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2023-05-04 06:30:09 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2023-05-04 06:30:09 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2023-05-04 06:30:09 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
            "2023-05-04 06:30:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-05-04 06:30:09 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2023-05-04 06:30:09 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2023-05-04 06:30:09 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2023-05-04 06:30:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 345\n",
            "epoch 001:   0% 0/345 [00:00<?, ?it/s]2023-05-04 06:30:10 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-05-04 06:30:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 344/345 [02:51<00:00,  1.98it/s, loss=3.871, nll_loss=2.196, ppl=4.58, wps=772.4, ups=2.02, wpb=382.2, bsz=15.1, num_updates=300, lr=2.3425e-06, gnorm=5.442, clip=100, train_wall=49, gb_free=7, wall=174]2023-05-04 06:33:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-05-04 06:33:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/141 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   1% 1/141 [00:00<00:18,  7.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 3/141 [00:00<00:10, 13.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 6/141 [00:00<00:07, 17.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 8/141 [00:00<00:07, 18.52it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   7% 10/141 [00:00<00:07, 18.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 12/141 [00:00<00:06, 18.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 15/141 [00:00<00:06, 20.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  13% 18/141 [00:00<00:05, 20.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 21/141 [00:01<00:05, 20.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 24/141 [00:01<00:05, 20.67it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  19% 27/141 [00:01<00:05, 20.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 30/141 [00:01<00:05, 20.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  23% 33/141 [00:01<00:05, 19.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 35/141 [00:01<00:05, 19.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 37/141 [00:01<00:05, 19.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  28% 40/141 [00:02<00:05, 19.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  30% 42/141 [00:02<00:05, 18.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  31% 44/141 [00:02<00:05, 19.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 46/141 [00:02<00:04, 19.05it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  34% 48/141 [00:02<00:05, 18.50it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 50/141 [00:02<00:04, 18.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  37% 52/141 [00:02<00:04, 18.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 54/141 [00:02<00:04, 18.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  40% 56/141 [00:02<00:04, 18.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41% 58/141 [00:03<00:04, 18.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 60/141 [00:03<00:04, 18.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 63/141 [00:03<00:04, 18.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 65/141 [00:03<00:04, 18.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 67/141 [00:03<00:03, 19.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  49% 69/141 [00:03<00:03, 18.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 71/141 [00:03<00:03, 18.70it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 73/141 [00:03<00:03, 18.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 75/141 [00:03<00:03, 17.81it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 77/141 [00:04<00:03, 17.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 79/141 [00:04<00:03, 17.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  57% 81/141 [00:04<00:03, 17.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 83/141 [00:04<00:03, 17.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 85/141 [00:04<00:03, 17.12it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 87/141 [00:04<00:03, 16.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  63% 89/141 [00:04<00:03, 16.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 91/141 [00:04<00:03, 16.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  66% 93/141 [00:05<00:02, 16.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 95/141 [00:05<00:02, 16.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  69% 97/141 [00:05<00:02, 16.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 99/141 [00:05<00:02, 16.70it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 101/141 [00:05<00:02, 16.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 103/141 [00:05<00:02, 16.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 105/141 [00:05<00:02, 17.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 107/141 [00:05<00:02, 16.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 109/141 [00:05<00:01, 16.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 111/141 [00:06<00:01, 16.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 113/141 [00:06<00:01, 16.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 115/141 [00:06<00:01, 17.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 117/141 [00:06<00:01, 17.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  84% 119/141 [00:06<00:01, 17.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86% 121/141 [00:06<00:01, 17.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  87% 123/141 [00:06<00:01, 17.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 125/141 [00:06<00:00, 17.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  90% 127/141 [00:07<00:00, 17.40it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 129/141 [00:07<00:00, 17.56it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 131/141 [00:07<00:00, 17.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 133/141 [00:07<00:00, 17.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 135/141 [00:07<00:00, 17.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  97% 137/141 [00:07<00:00, 17.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  99% 139/141 [00:07<00:00, 17.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 141/141 [00:07<00:00, 17.83it/s]\u001b[A\n",
            "                                                                          \u001b[A2023-05-04 06:33:10 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.524 | nll_loss 1.757 | ppl 3.38 | wps 3230.7 | wpb 177.1 | bsz 7.1 | num_updates 345\n",
            "2023-05-04 06:33:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 345 updates\n",
            "2023-05-04 06:33:10 | INFO | fairseq.trainer | Saving checkpoint to /content/finetuning/dataset_finetune/model/checkpoint1.pt\n",
            "2023-05-04 06:33:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/finetuning/dataset_finetune/model/checkpoint1.pt\n",
            "2023-05-04 06:34:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../dataset_finetune/model/checkpoint1.pt (epoch 1 @ 345 updates, score 3.524) (writing took 79.46397931900015 seconds)\n",
            "2023-05-04 06:34:29 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-05-04 06:34:29 | INFO | train | epoch 001 | loss 3.89 | nll_loss 2.216 | ppl 4.65 | wps 508.1 | ups 1.35 | wpb 376.7 | bsz 15 | num_updates 345 | lr 2.67888e-06 | gnorm 5.511 | clip 100 | train_wall 169 | gb_free 7.2 | wall 284\n",
            "2023-05-04 06:34:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-05-04 06:34:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 345\n",
            "epoch 002:   0% 0/345 [00:00<?, ?it/s]2023-05-04 06:34:29 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-05-04 06:34:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  98% 337/345 [02:45<00:03,  2.05it/s, loss=3.857, nll_loss=2.18, ppl=4.53, wps=744.9, ups=2.07, wpb=360.5, bsz=14.5, num_updates=600, lr=4.585e-06, gnorm=5.441, clip=100, train_wall=48, gb_free=7.1, wall=409]"
          ]
        }
      ],
      "source": [
        "!( fairseq-train ../dataset_finetune/final_bin \\\n",
        "--max-source-positions=210 \\\n",
        "--max-target-positions=210 \\\n",
        "--max-update=1000 \\\n",
        "--save-interval=1 \\\n",
        "--arch=transformer_4x \\\n",
        "--criterion=label_smoothed_cross_entropy \\\n",
        "--source-lang=SRC \\\n",
        "--lr-scheduler=inverse_sqrt \\\n",
        "--target-lang=TGT \\\n",
        "--label-smoothing=0.1 \\\n",
        "--optimizer adam \\\n",
        "--adam-betas \"(0.9, 0.98)\" \\\n",
        "--clip-norm 1.0 \\\n",
        "--warmup-init-lr 1e-07 \\\n",
        "--warmup-updates 4000 \\\n",
        "--dropout 0.2 \\\n",
        "--tensorboard-logdir ../dataset_finetune/tensorboard-wandb \\\n",
        "--save-dir ../dataset_finetune/model \\\n",
        "--keep-last-epochs 5 \\\n",
        "--patience 5 \\\n",
        "--skip-invalid-size-inputs-valid-test \\\n",
        "--user-dir model_configs \\\n",
        "--update-freq=2 \\\n",
        "--distributed-world-size 1 \\\n",
        "--max-tokens 256 \\\n",
        "--lr 3e-5 \\\n",
        "--restore-file ../en-indic/model/checkpoint_best.pt \\\n",
        "--reset-lr-scheduler \\\n",
        "--reset-meters \\\n",
        "--reset-dataloader \\\n",
        "--reset-optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from finetuning.indicTrans.inference.engine import Model\n",
        "en2indic_model_base = Model(expdir='/content/finetuning/en-indic')\n",
        "en2indic_model_finetuned = Model(expdir='/content/finetuning/dataset_finetune')"
      ],
      "metadata": {
        "id": "5AIRu2RtGczR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# checkpoint = torch.load('/content/finetuning/dataset/model/checkpoint_best.pt')"
      ],
      "metadata": {
        "id": "xWGvrEGoFDpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# en2indic_model_finetuned.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "id": "If9onI2MHytK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f=open(\"/content/finetuning/dataset_finetune/test/test.en\",\"r\")\n",
        "test_en_list_full=[]\n",
        "for x in f:\n",
        "  test_en_list_full.append(x)\n",
        "\n",
        "test_en_list=test_en_list_full\n",
        "\n",
        "f=open(\"/content/finetuning/dataset_finetune/test/test.mr\",\"r\")\n",
        "test_mr_list_full=[]\n",
        "for x in f:\n",
        "  test_mr_list_full.append(x)\n",
        "\n",
        "test_mr_list=test_mr_list_full"
      ],
      "metadata": {
        "id": "54pauC9dl8fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2\n",
        "NUM_BATCHES = len(test_en_list)//BATCH_SIZE + 1\n",
        "predictions_mr_finetuned=[]\n",
        "predictions_mr_base=[]\n",
        "for i in range(NUM_BATCHES):\n",
        "  en_sents_batch = test_en_list[BATCH_SIZE*i : BATCH_SIZE*(i+1)]\n",
        "  predictions_mr_base.extend(en2indic_model_base.batch_translate(en_sents_batch,'en','mr'))\n",
        "  predictions_mr_finetuned.extend(en2indic_model_finetuned.batch_translate(en_sents_batch,'en','mr'))"
      ],
      "metadata": {
        "id": "4HfcY1xNmCie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref_mr_list=[[x] for x in test_mr_list]"
      ],
      "metadata": {
        "id": "5PT6SJYPmL_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu.metrics import BLEU\n",
        "bleu = BLEU()"
      ],
      "metadata": {
        "id": "o5kIeDBxmRW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu.corpus_score(predictions_mr_base, ref_mr_list)"
      ],
      "metadata": {
        "id": "iuDElw7ARfeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu.corpus_score(predictions_mr_finetuned, ref_mr_list)"
      ],
      "metadata": {
        "id": "5qzd4s1bS3_B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}